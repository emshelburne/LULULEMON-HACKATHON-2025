{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f45f844-7678-47d9-81c9-492d8571f422",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9ac48e06-1b3a-4dd8-ba25-f8cc9daee548",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# ============ General ============\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Optional, List\n",
    "import math\n",
    "from itertools import chain\n",
    "\n",
    "# ============ Plotting ============\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "# ============ Text Preprocessing  ============\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# spaCy for lemmatization/POS filtering\n",
    "try:\n",
    "    import spacy\n",
    "    _SPACY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    _SPACY_AVAILABLE = False\n",
    "\n",
    "# ============ BERTopic stack ============\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Repro + warnings\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "# for _res in [\"stopwords\", \"wordnet\", \"omw-1.4\", \"punkt\"]:\n",
    "#     try:\n",
    "#         nltk.data.find(f\"corpora/{_res}\")\n",
    "#     except LookupError:\n",
    "#         nltk.download(_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfa28980-b7fb-4c70-92dd-ff0903377914",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set user's data path\n",
    "\n",
    "PATH = f\"C:/Users/emshe/Desktop/BRAINSTATION/LULULEMON/DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3de349-221d-42dd-92ac-bf8a5066d110",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Download NLTK files (run once)\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7c212-fa67-40e1-872f-bfd63fe1382a",
   "metadata": {},
   "source": [
    "## Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7ec57d-8299-43dd-9343-252d47d0bae0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "\n",
    "def clean_text(s: str | None) -> str | None:\n",
    "    \n",
    "    '''\n",
    "    Clean string by substituting spaces for problematic characters\n",
    "    '''\n",
    "    \n",
    "    if s is None:\n",
    "        return None\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b388f449-e518-4bf3-8680-e6024a8e3608",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to get datetime from UTC timestamp\n",
    "\n",
    "def dt_from_epoch(ts: Optional[int]):\n",
    "\n",
    "    \"\"\"\n",
    "    Convert timestamp to pd.datetime format\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if ts is None:\n",
    "        return None\n",
    "    return pd.to_datetime(ts, unit=\"s\", utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31afab8-4692-43da-87bc-0723484c41b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to examine dataframes\n",
    "\n",
    "def examine_df(name,df,\n",
    "               include_stats = True,\n",
    "               include_sample = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check basic info about a dataframe df\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n\\nNumber of records in the {name} is: {len(df)}\\n\")\n",
    "    print(f\"\\nNumber of features in the {name} is: {len(df.columns)}\\n\")\n",
    "    print(f\"The columns in the {name} are: {df.columns}\\n\")\n",
    "    print(f\"\\n Other info about {name}:\\n\")\n",
    "    display(df.info())\n",
    "    if include_stats == True:\n",
    "        print(f'\\n Basic statistical info about {name}:\\n')\n",
    "        display(df.describe())\n",
    "    if include_sample == True:\n",
    "        print(f\"\\n\\nSample of records in the {name}:\")\n",
    "        display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e964fb-cd5c-4f31-a3de-b34833a66d2d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to get sample from text column\n",
    "\n",
    "def get_text_samples(df: pd.DataFrame, text_col: str, n: int) -> None:\n",
    "\n",
    "    '''\n",
    "    Print n samples from a text column in a dataframe\n",
    "    '''\n",
    "\n",
    "    # Ensure pandas doesn't truncate text\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    # Sample and print 5 full negative reviews\n",
    "    print(\"Sample text data:\\n\\n\")\n",
    "    sample = df[text_col].sample(n)\n",
    "    for i, description in enumerate(sample, 1):\n",
    "        print(f\"Text sample {i}:\\n\\n\\n{description}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37be0d6-5f22-4ed3-87e1-b4a69677e0b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function for categorical bar graph\n",
    "\n",
    "def bar_graph(df: pd.DataFrame, col: str) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Generate bar graph for categorical column in a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in dataframe\")\n",
    "\n",
    "    counts = posts_df[col].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(f\"Distribution of {col.title()}\")\n",
    "    plt.xlabel(f\"{col.title()}\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8ae594-562e-49e5-a6e3-0f7c126993ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define function to plot histogram for numeric columns\n",
    "\n",
    "def histogram(df: pd.DataFrame, \n",
    "             col: str,\n",
    "            bins: int = 30,\n",
    "             log: bool = False) -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a histogram for a numeric column in a dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in dataframe\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    df[col].dropna().hist(bins=bins, edgecolor=\"black\", log=log)\n",
    "    plt.title(f\"Histogram of {col.title()}\")\n",
    "    plt.xlabel(col.title())\n",
    "    plt.ylabel(\"Log(Frequency)\" if log else \"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a314a84b-a9ef-4b8c-b3f7-6fc4a06bb7b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to load ndjson\n",
    "\n",
    "def load_plain_ndjson(path: str, limit: Optional[int] = None) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Load a plain-text NDJSON file line by line into a DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            rows.append({\n",
    "                \"post_id\": obj.get(\"id\"),\n",
    "                \"timestamp\": dt_from_epoch(obj.get(\"created_utc\")),\n",
    "                \"author\": obj.get(\"author\"),\n",
    "                \"title\": obj.get(\"title\"),\n",
    "                \"text\": obj.get(\"selftext\"),\n",
    "                \"score\": obj.get(\"score\"),\n",
    "                \"num_comments\": obj.get(\"num_comments\"),\n",
    "                \"permalink\": obj.get(\"permalink\"),\n",
    "                \"subreddit\": obj.get(\"subreddit\"),\n",
    "            })\n",
    "\n",
    "            if limit and i >= limit:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc6245-b542-4dd5-8e0d-0f05a02f765f",
   "metadata": {},
   "source": [
    "## Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efffc6ad-ecdd-4a46-8730-a7ea402b83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean data\n",
    "\n",
    "lulu_df = pd.read_parquet(f\"{PATH}/lululemon_submissions_clean.parquet\", engine = 'fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0925c71e-8b94-41d1-a28b-f6df482c8149",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of records in the lulu dataframe is: 57984\n",
      "\n",
      "\n",
      "Number of features in the lulu dataframe is: 6\n",
      "\n",
      "The columns in the lulu dataframe are: Index(['post_id', 'timestamp', 'title', 'text', 'score', 'num_comments'], dtype='object')\n",
      "\n",
      "\n",
      " Other info about lulu dataframe:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57984 entries, 0 to 57983\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype              \n",
      "---  ------        --------------  -----              \n",
      " 0   post_id       57984 non-null  object             \n",
      " 1   timestamp     57984 non-null  datetime64[ns, UTC]\n",
      " 2   title         57984 non-null  object             \n",
      " 3   text          57984 non-null  object             \n",
      " 4   score         57984 non-null  int64              \n",
      " 5   num_comments  57984 non-null  int64              \n",
      "dtypes: datetime64[ns, UTC](1), int64(2), object(3)\n",
      "memory usage: 2.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Basic statistical info about lulu dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>57984.000000</td>\n",
       "      <td>57984.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.446071</td>\n",
       "      <td>14.705126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>87.240166</td>\n",
       "      <td>40.279924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11864.000000</td>\n",
       "      <td>1987.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score  num_comments\n",
       "count  57984.000000  57984.000000\n",
       "mean      23.446071     14.705126\n",
       "std       87.240166     40.279924\n",
       "min        0.000000      0.000000\n",
       "25%        1.000000      2.000000\n",
       "50%        3.000000      6.000000\n",
       "75%       13.000000     13.000000\n",
       "max    11864.000000   1987.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sample of records in the lulu dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eielly</td>\n",
       "      <td>2020-01-01 05:33:25+00:00</td>\n",
       "      <td>Monthly Sales Post- January</td>\n",
       "      <td>FS: Aligns sz 4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eii06s</td>\n",
       "      <td>2020-01-01 12:46:35+00:00</td>\n",
       "      <td>Major problem falling down leggings?</td>\n",
       "      <td>Hello, over the last year I have been ordering lululemon stuff online as there is no store nearby. I have been trying different sizes and overall I feel their leggings are pretty bad at holding up. Especially wunder under and all the right places. I have wu in luon which is fine, and aligns are ok, new in movement seem to be ok. But align and luon get pilling issues, in movement fabric gets dust/feather sticking to it after 2 wears. In the same the luxtreme fabric in wunder under.. impossible. I don't feel i will get in the smaller size, as my thighs are huge (smaller waist).  Meanwhile I tried leggings from other brands... Eg Alo yoga, very similar model (extreme high waist airlift vs super high waist wu ) and the others are performing great. In alo i have leggings in s,m and l size,none is falling. I think i am getting really disappointed. It's very addictive to shop from lululemon and they look great in the mirror, but at the same time I cannot see the worth in comfort and performance during practice (yoga, both classical and more powerful,fitness) . Am I doing something wrong? I am afraid to invest even more $$, to try more models and risk having the same flows... I am also considering redesigning WU, adding a stich or elastic band to their waist..anyone having experience with that?</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eijtca</td>\n",
       "      <td>2020-01-01 16:00:56+00:00</td>\n",
       "      <td>Tops for yoga</td>\n",
       "      <td>I have a couple swiftly tech racerbacks for hot yoga and just ordered a swiftly breeze tank because I’m wanting something with a bit more coverage (higher neckline). I find I’m adjusting my racerbacks more than I’d like in yoga but will keep wearing for hot yoga since they wick sweat so well. \\n\\nWhat are everyone’s favorite tops/tanks to wear for yoga? I’m a 34D so I worry about having enough coverage and not falling out of bras and tops. I have the free to be serene bra but I don’t like wearing it for yoga for this reason. Would love to find a great top (preferably a tank) that I don’t need to adjust or worry about with all the forward folds and down dogs. I knew lulu makes high neck bras but I don’t like the idea of something tight across my collarbone.</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eikiew</td>\n",
       "      <td>2020-01-01 16:59:27+00:00</td>\n",
       "      <td>ABC Pants - Sizing</td>\n",
       "      <td>Hey all,\\n\\nI recently received ABC pants (size 32) from the store, and they fit well. However, I’ve heard from multiple friends that they lose their shape and stretch out/can become baggy after a couple of weeks-months of wear. Should I go back to the store and try a size 31 in anticipation of this? \\n\\nIf not, will lululemon replace these pants should they lose their shape? \\n\\nThanks!</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eil4bb</td>\n",
       "      <td>2020-01-01 17:46:20+00:00</td>\n",
       "      <td>Certain Aligns colours with thicker fabric?</td>\n",
       "      <td>Hi lemonheads :D\\n\\nI was wondering if anyone has bought a pair of aligns that seems to be made from a thicker fabric than the usual Nulu. I think I saw a post a while ago where a few people thought their aligns were thicker, they were all dark red colour but I don't remember the exact name (not Garnet!)\\n\\nI recently purchased a pair of full length Aligns off Poshmark in the colour Graphite, but they also seem to be thicker, almost luon. I thought maybe they were WUs, but don't believe WUs come in that colour? I've attached a photo of the size dot (it looks real to me?), though I'm not sure how to read it or where I would check. Any advice or thoughts would be great! \\n\\nhttps://preview.redd.it/8zq5q04lf7841.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=acaa777817edaeacc94b1effefd42b6f3d4f8c18</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                 timestamp  \\\n",
       "0  eielly 2020-01-01 05:33:25+00:00   \n",
       "1  eii06s 2020-01-01 12:46:35+00:00   \n",
       "2  eijtca 2020-01-01 16:00:56+00:00   \n",
       "3  eikiew 2020-01-01 16:59:27+00:00   \n",
       "4  eil4bb 2020-01-01 17:46:20+00:00   \n",
       "\n",
       "                                         title  \\\n",
       "0                  Monthly Sales Post- January   \n",
       "1         Major problem falling down leggings?   \n",
       "2                                Tops for yoga   \n",
       "3                           ABC Pants - Sizing   \n",
       "4  Certain Aligns colours with thicker fabric?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FS: Aligns sz 4   \n",
       "1  Hello, over the last year I have been ordering lululemon stuff online as there is no store nearby. I have been trying different sizes and overall I feel their leggings are pretty bad at holding up. Especially wunder under and all the right places. I have wu in luon which is fine, and aligns are ok, new in movement seem to be ok. But align and luon get pilling issues, in movement fabric gets dust/feather sticking to it after 2 wears. In the same the luxtreme fabric in wunder under.. impossible. I don't feel i will get in the smaller size, as my thighs are huge (smaller waist).  Meanwhile I tried leggings from other brands... Eg Alo yoga, very similar model (extreme high waist airlift vs super high waist wu ) and the others are performing great. In alo i have leggings in s,m and l size,none is falling. I think i am getting really disappointed. It's very addictive to shop from lululemon and they look great in the mirror, but at the same time I cannot see the worth in comfort and performance during practice (yoga, both classical and more powerful,fitness) . Am I doing something wrong? I am afraid to invest even more $$, to try more models and risk having the same flows... I am also considering redesigning WU, adding a stich or elastic band to their waist..anyone having experience with that?   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               I have a couple swiftly tech racerbacks for hot yoga and just ordered a swiftly breeze tank because I’m wanting something with a bit more coverage (higher neckline). I find I’m adjusting my racerbacks more than I’d like in yoga but will keep wearing for hot yoga since they wick sweat so well. \\n\\nWhat are everyone’s favorite tops/tanks to wear for yoga? I’m a 34D so I worry about having enough coverage and not falling out of bras and tops. I have the free to be serene bra but I don’t like wearing it for yoga for this reason. Would love to find a great top (preferably a tank) that I don’t need to adjust or worry about with all the forward folds and down dogs. I knew lulu makes high neck bras but I don’t like the idea of something tight across my collarbone.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Hey all,\\n\\nI recently received ABC pants (size 32) from the store, and they fit well. However, I’ve heard from multiple friends that they lose their shape and stretch out/can become baggy after a couple of weeks-months of wear. Should I go back to the store and try a size 31 in anticipation of this? \\n\\nIf not, will lululemon replace these pants should they lose their shape? \\n\\nThanks!   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Hi lemonheads :D\\n\\nI was wondering if anyone has bought a pair of aligns that seems to be made from a thicker fabric than the usual Nulu. I think I saw a post a while ago where a few people thought their aligns were thicker, they were all dark red colour but I don't remember the exact name (not Garnet!)\\n\\nI recently purchased a pair of full length Aligns off Poshmark in the colour Graphite, but they also seem to be thicker, almost luon. I thought maybe they were WUs, but don't believe WUs come in that colour? I've attached a photo of the size dot (it looks real to me?), though I'm not sure how to read it or where I would check. Any advice or thoughts would be great! \\n\\nhttps://preview.redd.it/8zq5q04lf7841.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=acaa777817edaeacc94b1effefd42b6f3d4f8c18   \n",
       "\n",
       "   score  num_comments  \n",
       "0      1             7  \n",
       "1      0             6  \n",
       "2      3             4  \n",
       "3      1             6  \n",
       "4      3            11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine data\n",
    "\n",
    "examine_df('lulu dataframe', lulu_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5177d54-a7f7-4cdd-9b92-68cfc30587a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Copy original dataframe\n",
    "\n",
    "og_lulu_df = lulu_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6023f0-b33e-459d-9c91-7bb61ba1f393",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ad46964f-0897-4e64-9146-698a27e71aaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reset dataframe\n",
    "\n",
    "lulu_df = og_lulu_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "73e6c995-fd90-4ae2-92fb-7abf829b6599",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define stop word list, lemmatizer, and regex\n",
    "\n",
    "# Stopwords\n",
    "custom_stop_words = [\n",
    "    # brand/boilerplate\n",
    "    \"lululemon\", \"lulu\", \"amp\", \"xx\", \"lol\",\n",
    "    \"like\", \"get\", \"got\", \"would\", \"anyone\", \"one\",\n",
    "\n",
    "    # deletion/removal artifacts\n",
    "    \"deleted\", \"remove\", \"removed\", \"removal\",\n",
    "    \"deleted_view\", \"removed_view\", \"view_poll\", \"poll_view\",\n",
    "    \"deleted_view_poll\", \"removed_view_poll\",\n",
    "    \"view\", \"poll\", \"results\", \"result\", \"vote\", \"votes\",\n",
    "    \"thread\", \"post\", \"posting\", \"posted\", \"comment\", \"comments\",\n",
    "\n",
    "    # generic low-information Reddit junk\n",
    "    \"http\", \"https\", \"www\", \"com\",\n",
    "    \"imgur\", \"jpg\", \"png\", \"gif\",\n",
    "    \"subreddit\", \"reddit\", \"mod\", \"mods\",\n",
    "    \"link\", \"links\",\n",
    "\n",
    "    # Scraped filler \n",
    "    \"user\", \"account\", \"profile\",\n",
    "    \"page\", \"site\", \"website\",\n",
    "    \"viewed\", \"views\", \"seen\"\n",
    "]\n",
    "\n",
    "base_stops = set(stopwords.words(\"english\"))\n",
    "base_stops -= {\"no\", \"nor\", \"not\", \"never\"}       # Keep negations\n",
    "\n",
    "stop_words = list(base_stops.union(custom_stop_words))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Precompile regex\n",
    "_link = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "_nonalpha = re.compile(r'[^a-z\\s]')\n",
    "_spaces = re.compile(r'\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f8d60460-b4d5-4a6b-8630-198dca17822d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define text preprocessor\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess text before modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = _link.sub(\" \", text)         # Remove links\n",
    "    text = _nonalpha.sub(\" \", text)     # Keep only letters/spaces\n",
    "    tokens = []\n",
    "    for t in text.split():\n",
    "        if t in stop_words or len(t) < 3:\n",
    "            continue\n",
    "        t = lemmatizer.lemmatize(t)\n",
    "        tokens.append(t)\n",
    "    return _spaces.sub(\" \", \" \".join(tokens)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "72a9bea5-1541-4a16-8e77-7f8beb48bb9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Apply text preprocessing\n",
    "\n",
    "lulu_df[\"clean_text\"] = lulu_df[\"title\"].fillna(\"\") + \" \" + lulu_df[\"text\"].fillna(\"\")\n",
    "lulu_df[\"clean_text\"] = lulu_df[\"clean_text\"].apply(preprocess)\n",
    "\n",
    "# drop docs with <5 tokens to reduce noise\n",
    "lulu_df = lulu_df[lulu_df[\"clean_text\"].str.split().str.len() >= 5].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "46d8da26-6a21-4296-9af5-c2328ec2da07",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text data:\n",
      "\n",
      "\n",
      "Text sample 1:\n",
      "\n",
      "\n",
      "iso asia fit angel foot tall fellow petite addict know hemming regular fit not work loose around ankle live sydney australia asia fit available look inch wunder unders longer stocked dying hand couple pair hoping someone help thank\n",
      "\n",
      "\n",
      "\n",
      "Text sample 2:\n",
      "\n",
      "\n",
      "intent jogger luon thinking picking pair\n",
      "\n",
      "\n",
      "\n",
      "Text sample 3:\n",
      "\n",
      "\n",
      "dollar giftcard contacted gec wanting exchange pair legging agent super precious decided offer egiftcard maybe inconvenience fee smt sent form fill normal never heard\n",
      "\n",
      "\n",
      "\n",
      "Text sample 4:\n",
      "\n",
      "\n",
      "sizing advice scuba full zip hoodies oversized regular fit long time lurker finally decided want hooded jacket autumn weather asking sizing advice full zip oversized full zip regular hoodie pretty short usually wear top also ideally hoodie cover waistband pant presentation might wanna throw top room cold regular fit look cover well proportion look throwing seems slim hold weight lower half muscly arm well not sure look good tight arm bottom near hip loose elsewhere model similar body shape tall lean hour glass shape really tell photo also wary sizing pretty short want sleeve overall length overwhelm frame still wanna use thumbhole hand oversized fit look pretty good loose place not sure cropped look look weird wearing longer blouse not wearing high waisted bottom also concerned may look slouchy searched sub people complaining excessively boxy wide look terrible petite frame unfortunately hoodies sold store near anything near try person buy photograph sizing advice especially photo fit body frame shorter picture hoodie zipped unzipped different length top rise pant appreciate much\n",
      "\n",
      "\n",
      "\n",
      "Text sample 5:\n",
      "\n",
      "\n",
      "alo morning went grand opening alo yoga month coveting item online felt weird cheating honest product feel mediocre wow went straight afterwards check newest wmtm item astounded much better item feel quality material comparison feel unfair fight overall product design function aesthetic seems possibly debatable curious else ventured try alo immediately went back store opened sneaker wanted stock not huge deal online order make two separate transaction store purchase online felt clunky happy keep coin alo basically day wondering people experience\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check some examples\n",
    "\n",
    "get_text_samples(lulu_df, 'clean_text', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "16994c30-0aa0-4fc8-9c39-54fdb700971b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to merge bigrams\n",
    "\n",
    "def merge_bigrams(doc, bigram_set, stop_words = stop_words):\n",
    "    \"\"\"\n",
    "    Merge bigrams, then remove any that are still in stop_words\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(doc):\n",
    "        if i < len(doc)-1 and (doc[i], doc[i+1]) in bigram_set:\n",
    "            candidate = f\"{doc[i]}_{doc[i+1]}\"\n",
    "            if candidate not in stop_words:\n",
    "                merged.append(candidate)\n",
    "            i += 2\n",
    "        else:\n",
    "            token = doc[i]\n",
    "            if token not in stop_words:\n",
    "                merged.append(token)\n",
    "            i += 1\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ad115157-1c09-4811-a52a-727a98f748aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Collect tokenized docs\n",
    "\n",
    "tokenized_docs = [doc.split() for doc in lulu_df[\"clean_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7ae769bd-05cb-49af-be18-116d1fe5b5c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Apply bigram detector\n",
    "\n",
    "bigrams = [list(ngrams(doc, 2)) for doc in tokenized_docs]\n",
    "flat_bigrams = [bg for doc in bigrams for bg in doc]\n",
    "bigram_counts = Counter(flat_bigrams)\n",
    "\n",
    "common_bigrams = {bg for bg, count in bigram_counts.items() if count >= 10}\n",
    "tokenized_bigrams = [merge_bigrams(doc, common_bigrams) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "de8ee4f7-834a-4222-ac7e-cfd09c469f9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Add bigram text back to dataframe\n",
    "\n",
    "lulu_df[\"clean_text_bigram\"] = [\" \".join(doc) for doc in tokenized_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "284f27ce-6c58-4933-9182-41c4dec0cede",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text data:\n",
      "\n",
      "\n",
      "Text sample 1:\n",
      "\n",
      "\n",
      "need_help fit_comparison ready_rulu jogger_align jogger_align jogger_size think_fit pretty_well not_much room butt ready_rulus compare align_jogger size_well size\n",
      "\n",
      "\n",
      "\n",
      "Text sample 2:\n",
      "\n",
      "\n",
      "visa card getting denied poking around sub determine possible sign pointed yes could_use visa_gift debit_card buy_stuff however keep_getting declined despite purchasing card today activated trying troubleshoot seems need assign billing_address visa card however went_back card add zip_code button nowhere_found guessing made stupid update regardless know workaround card accepted card purchased target back_card mybalancenow\n",
      "\n",
      "\n",
      "\n",
      "Text sample 3:\n",
      "\n",
      "\n",
      "lounge legging_curious else_disappointed current selection legging exception aligns_seem workout type_legging fairly similar hoping_find cotton lounge legging_maybe luon_material seems right everything slick spandexy material_aligns great super delicate everyday_wear moved away cotton loungy legging_seems almost_exclusively geared towards workout\n",
      "\n",
      "\n",
      "\n",
      "Text sample 4:\n",
      "\n",
      "\n",
      "jogger care pair_surge jogger okay put_dryer jogger owned past seem wear thin lose_shape pretty_quickly appreciate tip_keep original condition long_possible\n",
      "\n",
      "\n",
      "\n",
      "Text sample 5:\n",
      "\n",
      "\n",
      "help_find color_color align_short vivd plum never color obsessed someone_please help_figure color\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check some examples\n",
    "\n",
    "get_text_samples(lulu_df, 'clean_text_bigram', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "40273aa0-8b33-468d-b6a2-b5df8fb14e8f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████| 884/884 [11:22<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Text used for embeddings: light cleaning only (keep function words, no underscores)\n",
    "\n",
    "embed_docs = (\n",
    "    lulu_df[\"title\"].fillna(\"\") + \" \" + lulu_df[\"text\"].fillna(\"\")\n",
    ").str.replace(r'https?://\\S+|www\\.\\S+', ' ', regex=True).str.strip()\n",
    "\n",
    "# Text used for c-TF-IDF (vectorizer): your bigram-merged tokens\n",
    "\n",
    "vectorizer_docs = lulu_df[\"clean_text_bigram\"].tolist()\n",
    "\n",
    "# Compute embeddings on embed_docs, but fit BERTopic on vectorizer_docs\n",
    "\n",
    "embs = st_model.encode(embed_docs.tolist(),\n",
    "                       batch_size=64, show_progress_bar=True, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63c400-9ffd-4ff1-9fac-c72130a4f429",
   "metadata": {},
   "source": [
    "## Modeling with BERTopic: Phase One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b02223ad-717e-4bee-8fac-ffe2741ef9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline sentence embedding model\n",
    "\n",
    "EMB_NAME = \"all-MiniLM-L6-v2\"\n",
    "st_model = SentenceTransformer(EMB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d71dba56-5c5f-45d8-a1b8-8997edfdcf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure backend UMAP + HDBSCAN models\n",
    "\n",
    "# Larger min_cluster_size = merge small clusters\n",
    "min_cluster_size = max(100, int(0.012 * N))  # 1% of corpus\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=40,     # ↑ = broader neighborhood → merge clusters\n",
    "    n_components=2,     # compress more tightly\n",
    "    min_dist=0.05,      # slight spread so clusters don’t overfragment\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=35,     # ↑ = stricter cluster acceptance, fewer outliers\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    cluster_selection_epsilon=0.1,  # gentle merging of border clusters\n",
    "    prediction_data=True,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ab543562-e52b-4e20-9b93-29b957a9f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectorizer to keep unigrams and bigrams\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "    stop_words=stop_words,\n",
    "    strip_accents=\"unicode\",\n",
    "    min_df=1,      # integer 1 is always safe when n_topics is small\n",
    "    max_df=.95,    # or 0.95 if you still want to cap very common terms\n",
    "    max_features=30000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "139b7a14-b8a6-4699-a2e3-7ea83a621576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BERTopic model\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=st_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language=\"english\",\n",
    "    calculate_probabilities=True,\n",
    "    top_n_words=15,\n",
    "    verbose=True,\n",
    "    nr_topics=None    # let clustering decide\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f3f8d524-2c5b-4c43-befb-81411c5ce901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 14:31:07,838 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-06 14:32:46,541 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-06 14:32:46,544 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-06 14:32:54,307 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-06 14:32:54,324 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-09-06 14:33:02,628 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full runtime: 1.92 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Fit BERTopic model\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "topics, probs = topic_model.fit_transform(vectorizer_docs, embs)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end-start)/60\n",
    "\n",
    "print(f\"Full runtime: {runtime:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "63c58a7f-7a31-41d2-8d41-33bfae998da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered topics (excl. -1): 2\n",
      "Outlier docs (-1): 0 / 56574 = 0.0%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>55739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count\n",
       "0      0  55739\n",
       "1      1    835"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine basic topic info\n",
    "\n",
    "# Topic table and basic stats\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.head(10)\n",
    "\n",
    "# Number of discovered topics (exclude -1 = outliers)\n",
    "n_topics = int((topic_info[\"Topic\"] != -1).sum())\n",
    "n_docs = len(docs)\n",
    "outlier_share = (topics.count(-1) / n_docs) if n_docs else 0.0\n",
    "\n",
    "print(f\"Discovered topics (excl. -1): {n_topics}\")\n",
    "print(f\"Outlier docs (-1): {topics.count(-1)} / {n_docs} = {outlier_share:.1%}\")\n",
    "\n",
    "# Topic size distribution\n",
    "topic_info[[\"Topic\", \"Count\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "264a6f3d-5ff1-4f79-b17f-ff9579d3b579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic diversity (for top 10 words for each topic): 1.000\n"
     ]
    }
   ],
   "source": [
    "# Check topic diversity\n",
    "\n",
    "TOP_N = 10\n",
    "words_per_topic = {\n",
    "    t: [w for w, _ in topic_model.get_topic(t)[:TOP_N]]\n",
    "    for t in topic_info[\"Topic\"].tolist() if t != -1\n",
    "}\n",
    "\n",
    "all_top_words = list(chain.from_iterable(words_per_topic.values()))\n",
    "diversity = len(set(all_top_words)) / max(1, len(all_top_words))\n",
    "print(f\"Topic diversity (for top {TOP_N} words for each topic): {diversity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bdaf1eac-83d6-4ae7-b041-11b4ac0ba378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Topic 0 | size=50714\n",
      "\n",
      "Top words: legging, pant, high_rise, aligns, jacket, bra, fast_free, tank, jogger, shirt, sizing, energy_bra, scuba, long_sleeve, white\n",
      "\n",
      "--- Ex1: usa_drop drop_start time_today pst bra_align reversible_bra black tigre_camo deep_coal multi_ebb street_bra green_fern energy_bra chroma multi_flow bra green_fern free_serene bra_lavender dew free_serene inflorescence_multi free_wild lavender_dew run_time bra desert_sun top tee ripened_raspberry bac...\n",
      "\n",
      "--- Ex2: collection review_wunder train instill align round collection step low_buy thought_share favourite attribute pro_con legging ive lucky acquire appreciating hopefully_help deciding style_wunder train recent legging_already became holy_grail material equally soft_align provides much_support not move w...\n",
      "\n",
      "--- Ex3: wonder woman_wear legging took pace_breaker hemmed feel dont give men train leg shorter short besides fast_free deciding wear hottie_hots hiking leg_day thinking winter utah men compression_legging annoying tried_fast free_aligns dance_studio omg wonder woman_wear comfy appreciate support holding ju...\n",
      "================================================================================\n",
      "\n",
      "Topic 1 | size=2323\n",
      "\n",
      "Top words: stain, wash, mat, washing, smell, washed, dye, water, white, detergent, dry, dryer, laundry, shrink, stained\n",
      "\n",
      "--- Ex1: washed smell mildew mixed detergent air_drying wash detergent hand dry dry always_seems smell mildew splash detergent making_want throw_dryer tip\n",
      "\n",
      "--- Ex2: black_legging stain threw legging degree mix wash detergent softener stain_not put water wash cleaning product please_help\n",
      "\n",
      "--- Ex3: laundry stain define washed pink_define recently seeing stain detergent stain already_tried oxyclean spray not_working\n",
      "================================================================================\n",
      "\n",
      "Topic 2 | size=1692\n",
      "\n",
      "Top words: ebb, everywhere_belt, city_adventurer, carry, everywhere_belt bag, tote, pouch, mini, wristlet, zipper, tote_bag, new_crew, dual_pouch, travel, laptop\n",
      "\n",
      "--- Ex1: difference city_adventurer original city_adventurer look_identical know_difference also carry laptop work_usually want_anything bulky mini good original_size two_store original\n",
      "\n",
      "--- Ex2: see_bought today everywhere_belt bag spiced_chai city_adventurer backpack_nano heritage way_bucket hat rwml xplr everywhere_belt bag spiced_chai city_adventurer backpack_nano heritage way_bucket hat rwml xplr told came_store try oversized_crew turned bringing cute thing home new_gear walking_dog hop...\n",
      "\n",
      "--- Ex3: dual_pouch wristlet everywhere_belt bag debating_whether not dual_pouch wristlet specifically pink_peony white_opal dropping today already use everywhere_belt bag daily although_think making quick run store_something pretty convenient everyone use prefer use together use separate\n"
     ]
    }
   ],
   "source": [
    "# Check top words for each topic\n",
    "\n",
    "# Top-k topics by size (excluding -1)\n",
    "topk = topic_info.query(\"Topic != -1\").head(10)[\"Topic\"].tolist()\n",
    "\n",
    "for t in topk:\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTopic {t} | size={int(topic_info.loc[topic_info['Topic']==t, 'Count'])}\")\n",
    "    print(\"\\nTop words:\", \", \".join([w for w, _ in topic_model.get_topic(t)[:15]]))\n",
    "    \n",
    "    # Representative examples\n",
    "    reps = topic_model.get_representative_docs(t)[:3]\n",
    "    for i, doc in enumerate(reps, 1):\n",
    "        preview = doc[:300].replace(\"\\n\", \" \")\n",
    "        suffix = \"...\" if len(doc) > 300 else \"\"\n",
    "        print(f\"\\n--- Ex{i}: {preview}{suffix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e5e8e-0060-47e8-b8df-e8c2eb4f0304",
   "metadata": {},
   "source": [
    "## Topic Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "88292876-71eb-464e-b5b8-d82015cb7c83",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[135]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Overall map of topics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\lulu_bert_env\\Lib\\site-packages\\bertopic\\_bertopic.py:2424\u001b[39m, in \u001b[36mBERTopic.visualize_topics\u001b[39m\u001b[34m(self, topics, top_n_topics, use_ctfidf, custom_labels, title, width, height)\u001b[39m\n\u001b[32m   2391\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Visualize topics, their sizes, and their corresponding words.\u001b[39;00m\n\u001b[32m   2392\u001b[39m \n\u001b[32m   2393\u001b[39m \u001b[33;03mThis visualization is highly inspired by LDAvis, a great visualization\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2421\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m   2422\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2423\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mplotting\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_topics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2425\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_n_topics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_n_topics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2428\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_ctfidf\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_ctfidf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2433\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\lulu_bert_env\\Lib\\site-packages\\bertopic\\plotting\\_topics.py:74\u001b[39m, in \u001b[36mvisualize_topics\u001b[39m\u001b[34m(topic_model, topics, top_n_topics, use_ctfidf, custom_labels, title, width, height)\u001b[39m\n\u001b[32m     72\u001b[39m     words = [topic_model.custom_labels_[topic + topic_model._outliers] \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topic_list]\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     words = \u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m | \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtopic_list\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Embed c-TF-IDF into 2D\u001b[39;00m\n\u001b[32m     77\u001b[39m all_topics = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(topic_model.get_topics().keys()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\lulu_bert_env\\Lib\\site-packages\\bertopic\\plotting\\_topics.py:74\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     72\u001b[39m     words = [topic_model.custom_labels_[topic + topic_model._outliers] \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topic_list]\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     words = [\u001b[33m\"\u001b[39m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m.join([word[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m[:\u001b[32m5\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topic_list]\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Embed c-TF-IDF into 2D\u001b[39;00m\n\u001b[32m     77\u001b[39m all_topics = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(topic_model.get_topics().keys()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\lulu_bert_env\\Lib\\site-packages\\bertopic\\_bertopic.py:1612\u001b[39m, in \u001b[36mBERTopic.get_topic\u001b[39m\u001b[34m(self, topic, full)\u001b[39m\n\u001b[32m   1596\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return top n words for a specific topic and their c-TF-IDF scores.\u001b[39;00m\n\u001b[32m   1597\u001b[39m \n\u001b[32m   1598\u001b[39m \u001b[33;03mArguments:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1609\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m   1610\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1611\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtopic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtopic_representations_\u001b[49m:\n\u001b[32m   1613\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m full:\n\u001b[32m   1614\u001b[39m         representations = {\u001b[33m\"\u001b[39m\u001b[33mMain\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.topic_representations_[topic]}\n",
      "\u001b[31mTypeError\u001b[39m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "# Overall map of topics\n",
    "\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "66c4da09-3e1b-4b0c-a40e-f18ccf06adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top words per topic (bar charts)\n",
    "\n",
    "topic_model.visualize_barchart(top_n_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "334e3538-8eb0-4793-90e1-6ad1a851f921",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualizie topic similarity heatmap\n",
    "\n",
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82a33566-3d9b-448f-b28a-fb2c01b69d12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualize hierarchy (dendrogram)\n",
    "\n",
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "52f0eec6-4942-499f-ba68-d3deccdda1b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save topics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_out = \u001b[43mdf\u001b[49m.copy()\n\u001b[32m      4\u001b[39m df_out[\u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m] = topics\n\u001b[32m      5\u001b[39m df_out[\u001b[33m\"\u001b[39m\u001b[33mtopic_prob_max\u001b[39m\u001b[33m\"\u001b[39m] = [\u001b[38;5;28mfloat\u001b[39m(p.max()) \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m probs]\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Save topics\n",
    "\n",
    "df_out = lulu_df.copy()\n",
    "df_out[\"topic\"] = topics\n",
    "df_out[\"topic_prob_max\"] = [float(p.max()) if p is not None else None for p in probs]\n",
    "\n",
    "# Optional: keep a cleaner subset for inspection\n",
    "cols_show = [\"topic\", \"topic_prob_max\"]\n",
    "if \"title\" in df_out:\n",
    "    cols_show = [\"title\"] + cols_show\n",
    "if \"subreddit\" in df_out.columns:\n",
    "    cols_show = [\"subreddit\"] + cols_show\n",
    "if \"created_utc\" in df_out.columns:\n",
    "    cols_show = [\"created_utc\"] + cols_show\n",
    "\n",
    "df_out[cols_show].head(10)\n",
    "\n",
    "# Save artifacts\n",
    "topic_info.to_csv(f\"{PATH}/bertopic_topic_info_00.csv\", index=False)\n",
    "df_out.to_csv(f\"{PATH}/reddit_posts_with_topics_00.csv\", index=False)\n",
    "print(\"Saved: bertopic_topic_info.csv, reddit_posts_with_topics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lulu_bert_env)",
   "language": "python",
   "name": "lulu_bert_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
