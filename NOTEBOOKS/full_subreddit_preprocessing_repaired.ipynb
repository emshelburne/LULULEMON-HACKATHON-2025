{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f45f844-7678-47d9-81c9-492d8571f422",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac48e06-1b3a-4dd8-ba25-f8cc9daee548",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# ============ General ============\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Optional, List\n",
    "import math\n",
    "from itertools import chain\n",
    "\n",
    "# ============ Plotting ============\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "# ============ Text Preprocessing  ============\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# spaCy for lemmatization/POS filtering\n",
    "try:\n",
    "    import spacy\n",
    "    _SPACY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    _SPACY_AVAILABLE = False\n",
    "\n",
    "# ============ BERTopic stack ============\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Repro + warnings\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "# for _res in [\"stopwords\", \"wordnet\", \"omw-1.4\", \"punkt\"]:\n",
    "#     try:\n",
    "#         nltk.data.find(f\"corpora/{_res}\")\n",
    "#     except LookupError:\n",
    "#         nltk.download(_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa28980-b7fb-4c70-92dd-ff0903377914",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set user's data path\n",
    "\n",
    "PATH = f\"C:/Users/emshe/Desktop/BRAINSTATION/LULULEMON/DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3de349-221d-42dd-92ac-bf8a5066d110",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Download NLTK files (run once)\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7c212-fa67-40e1-872f-bfd63fe1382a",
   "metadata": {},
   "source": [
    "## Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7ec57d-8299-43dd-9343-252d47d0bae0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "\n",
    "def clean_text(s: str | None) -> str | None:\n",
    "    \n",
    "    '''\n",
    "    Clean string by substituting spaces for problematic characters\n",
    "    '''\n",
    "    \n",
    "    if s is None:\n",
    "        return None\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b388f449-e518-4bf3-8680-e6024a8e3608",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to get datetime from UTC timestamp\n",
    "\n",
    "def dt_from_epoch(ts: Optional[int]):\n",
    "\n",
    "    \"\"\"\n",
    "    Convert timestamp to pd.datetime format\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if ts is None:\n",
    "        return None\n",
    "    return pd.to_datetime(ts, unit=\"s\", utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31afab8-4692-43da-87bc-0723484c41b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to examine dataframes\n",
    "\n",
    "def examine_df(name,df,\n",
    "               include_stats = True,\n",
    "               include_sample = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check basic info about a dataframe df\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n\\nNumber of records in the {name} is: {len(df)}\\n\")\n",
    "    print(f\"\\nNumber of features in the {name} is: {len(df.columns)}\\n\")\n",
    "    print(f\"The columns in the {name} are: {df.columns}\\n\")\n",
    "    print(f\"\\n Other info about {name}:\\n\")\n",
    "    display(df.info())\n",
    "    if include_stats == True:\n",
    "        print(f'\\n Basic statistical info about {name}:\\n')\n",
    "        display(df.describe())\n",
    "    if include_sample == True:\n",
    "        print(f\"\\n\\nSample of records in the {name}:\")\n",
    "        display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e964fb-cd5c-4f31-a3de-b34833a66d2d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to get sample from text column\n",
    "\n",
    "def get_text_samples(df: pd.DataFrame, text_col: str, n: int) -> None:\n",
    "\n",
    "    '''\n",
    "    Print n samples from a text column in a dataframe\n",
    "    '''\n",
    "\n",
    "    # Ensure pandas doesn't truncate text\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    # Sample and print 5 full negative reviews\n",
    "    print(\"Sample text data:\\n\\n\")\n",
    "    sample = df[text_col].sample(n)\n",
    "    for i, description in enumerate(sample, 1):\n",
    "        print(f\"Text sample {i}:\\n\\n\\n{description}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37be0d6-5f22-4ed3-87e1-b4a69677e0b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function for categorical bar graph\n",
    "\n",
    "def bar_graph(df: pd.DataFrame, col: str) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Generate bar graph for categorical column in a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in dataframe\")\n",
    "\n",
    "    counts = posts_df[col].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(f\"Distribution of {col.title()}\")\n",
    "    plt.xlabel(f\"{col.title()}\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8ae594-562e-49e5-a6e3-0f7c126993ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define function to plot histogram for numeric columns\n",
    "\n",
    "def histogram(df: pd.DataFrame, \n",
    "             col: str,\n",
    "            bins: int = 30,\n",
    "             log: bool = False) -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a histogram for a numeric column in a dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in dataframe\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    df[col].dropna().hist(bins=bins, edgecolor=\"black\", log=log)\n",
    "    plt.title(f\"Histogram of {col.title()}\")\n",
    "    plt.xlabel(col.title())\n",
    "    plt.ylabel(\"Log(Frequency)\" if log else \"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a314a84b-a9ef-4b8c-b3f7-6fc4a06bb7b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to load ndjson\n",
    "\n",
    "def load_plain_ndjson(path: str, limit: Optional[int] = None) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Load a plain-text NDJSON file line by line into a DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            rows.append({\n",
    "                \"post_id\": obj.get(\"id\"),\n",
    "                \"timestamp\": dt_from_epoch(obj.get(\"created_utc\")),\n",
    "                \"author\": obj.get(\"author\"),\n",
    "                \"title\": obj.get(\"title\"),\n",
    "                \"text\": obj.get(\"selftext\"),\n",
    "                \"score\": obj.get(\"score\"),\n",
    "                \"num_comments\": obj.get(\"num_comments\"),\n",
    "                \"permalink\": obj.get(\"permalink\"),\n",
    "                \"subreddit\": obj.get(\"subreddit\"),\n",
    "            })\n",
    "\n",
    "            if limit and i >= limit:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc6245-b542-4dd5-8e0d-0f05a02f765f",
   "metadata": {},
   "source": [
    "## Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efffc6ad-ecdd-4a46-8730-a7ea402b83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean data\n",
    "\n",
    "lulu_df = pd.read_parquet(f\"{PATH}/lululemon_submissions_clean.parquet\", engine = 'fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0925c71e-8b94-41d1-a28b-f6df482c8149",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of records in the lulu dataframe is: 57984\n",
      "\n",
      "\n",
      "Number of features in the lulu dataframe is: 6\n",
      "\n",
      "The columns in the lulu dataframe are: Index(['post_id', 'timestamp', 'title', 'text', 'score', 'num_comments'], dtype='object')\n",
      "\n",
      "\n",
      " Other info about lulu dataframe:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57984 entries, 0 to 57983\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype              \n",
      "---  ------        --------------  -----              \n",
      " 0   post_id       57984 non-null  object             \n",
      " 1   timestamp     57984 non-null  datetime64[ns, UTC]\n",
      " 2   title         57984 non-null  object             \n",
      " 3   text          57984 non-null  object             \n",
      " 4   score         57984 non-null  int64              \n",
      " 5   num_comments  57984 non-null  int64              \n",
      "dtypes: datetime64[ns, UTC](1), int64(2), object(3)\n",
      "memory usage: 2.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Basic statistical info about lulu dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>57984.000000</td>\n",
       "      <td>57984.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.446071</td>\n",
       "      <td>14.705126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>87.240166</td>\n",
       "      <td>40.279924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11864.000000</td>\n",
       "      <td>1987.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score  num_comments\n",
       "count  57984.000000  57984.000000\n",
       "mean      23.446071     14.705126\n",
       "std       87.240166     40.279924\n",
       "min        0.000000      0.000000\n",
       "25%        1.000000      2.000000\n",
       "50%        3.000000      6.000000\n",
       "75%       13.000000     13.000000\n",
       "max    11864.000000   1987.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sample of records in the lulu dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eielly</td>\n",
       "      <td>2020-01-01 05:33:25+00:00</td>\n",
       "      <td>Monthly Sales Post- January</td>\n",
       "      <td>FS: Aligns sz 4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eii06s</td>\n",
       "      <td>2020-01-01 12:46:35+00:00</td>\n",
       "      <td>Major problem falling down leggings?</td>\n",
       "      <td>Hello, over the last year I have been ordering lululemon stuff online as there is no store nearby. I have been trying different sizes and overall I feel their leggings are pretty bad at holding up. Especially wunder under and all the right places. I have wu in luon which is fine, and aligns are ok, new in movement seem to be ok. But align and luon get pilling issues, in movement fabric gets dust/feather sticking to it after 2 wears. In the same the luxtreme fabric in wunder under.. impossible. I don't feel i will get in the smaller size, as my thighs are huge (smaller waist).  Meanwhile I tried leggings from other brands... Eg Alo yoga, very similar model (extreme high waist airlift vs super high waist wu ) and the others are performing great. In alo i have leggings in s,m and l size,none is falling. I think i am getting really disappointed. It's very addictive to shop from lululemon and they look great in the mirror, but at the same time I cannot see the worth in comfort and performance during practice (yoga, both classical and more powerful,fitness) . Am I doing something wrong? I am afraid to invest even more $$, to try more models and risk having the same flows... I am also considering redesigning WU, adding a stich or elastic band to their waist..anyone having experience with that?</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eijtca</td>\n",
       "      <td>2020-01-01 16:00:56+00:00</td>\n",
       "      <td>Tops for yoga</td>\n",
       "      <td>I have a couple swiftly tech racerbacks for hot yoga and just ordered a swiftly breeze tank because I’m wanting something with a bit more coverage (higher neckline). I find I’m adjusting my racerbacks more than I’d like in yoga but will keep wearing for hot yoga since they wick sweat so well. \\n\\nWhat are everyone’s favorite tops/tanks to wear for yoga? I’m a 34D so I worry about having enough coverage and not falling out of bras and tops. I have the free to be serene bra but I don’t like wearing it for yoga for this reason. Would love to find a great top (preferably a tank) that I don’t need to adjust or worry about with all the forward folds and down dogs. I knew lulu makes high neck bras but I don’t like the idea of something tight across my collarbone.</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eikiew</td>\n",
       "      <td>2020-01-01 16:59:27+00:00</td>\n",
       "      <td>ABC Pants - Sizing</td>\n",
       "      <td>Hey all,\\n\\nI recently received ABC pants (size 32) from the store, and they fit well. However, I’ve heard from multiple friends that they lose their shape and stretch out/can become baggy after a couple of weeks-months of wear. Should I go back to the store and try a size 31 in anticipation of this? \\n\\nIf not, will lululemon replace these pants should they lose their shape? \\n\\nThanks!</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eil4bb</td>\n",
       "      <td>2020-01-01 17:46:20+00:00</td>\n",
       "      <td>Certain Aligns colours with thicker fabric?</td>\n",
       "      <td>Hi lemonheads :D\\n\\nI was wondering if anyone has bought a pair of aligns that seems to be made from a thicker fabric than the usual Nulu. I think I saw a post a while ago where a few people thought their aligns were thicker, they were all dark red colour but I don't remember the exact name (not Garnet!)\\n\\nI recently purchased a pair of full length Aligns off Poshmark in the colour Graphite, but they also seem to be thicker, almost luon. I thought maybe they were WUs, but don't believe WUs come in that colour? I've attached a photo of the size dot (it looks real to me?), though I'm not sure how to read it or where I would check. Any advice or thoughts would be great! \\n\\nhttps://preview.redd.it/8zq5q04lf7841.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=acaa777817edaeacc94b1effefd42b6f3d4f8c18</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                 timestamp  \\\n",
       "0  eielly 2020-01-01 05:33:25+00:00   \n",
       "1  eii06s 2020-01-01 12:46:35+00:00   \n",
       "2  eijtca 2020-01-01 16:00:56+00:00   \n",
       "3  eikiew 2020-01-01 16:59:27+00:00   \n",
       "4  eil4bb 2020-01-01 17:46:20+00:00   \n",
       "\n",
       "                                         title  \\\n",
       "0                  Monthly Sales Post- January   \n",
       "1         Major problem falling down leggings?   \n",
       "2                                Tops for yoga   \n",
       "3                           ABC Pants - Sizing   \n",
       "4  Certain Aligns colours with thicker fabric?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FS: Aligns sz 4   \n",
       "1  Hello, over the last year I have been ordering lululemon stuff online as there is no store nearby. I have been trying different sizes and overall I feel their leggings are pretty bad at holding up. Especially wunder under and all the right places. I have wu in luon which is fine, and aligns are ok, new in movement seem to be ok. But align and luon get pilling issues, in movement fabric gets dust/feather sticking to it after 2 wears. In the same the luxtreme fabric in wunder under.. impossible. I don't feel i will get in the smaller size, as my thighs are huge (smaller waist).  Meanwhile I tried leggings from other brands... Eg Alo yoga, very similar model (extreme high waist airlift vs super high waist wu ) and the others are performing great. In alo i have leggings in s,m and l size,none is falling. I think i am getting really disappointed. It's very addictive to shop from lululemon and they look great in the mirror, but at the same time I cannot see the worth in comfort and performance during practice (yoga, both classical and more powerful,fitness) . Am I doing something wrong? I am afraid to invest even more $$, to try more models and risk having the same flows... I am also considering redesigning WU, adding a stich or elastic band to their waist..anyone having experience with that?   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               I have a couple swiftly tech racerbacks for hot yoga and just ordered a swiftly breeze tank because I’m wanting something with a bit more coverage (higher neckline). I find I’m adjusting my racerbacks more than I’d like in yoga but will keep wearing for hot yoga since they wick sweat so well. \\n\\nWhat are everyone’s favorite tops/tanks to wear for yoga? I’m a 34D so I worry about having enough coverage and not falling out of bras and tops. I have the free to be serene bra but I don’t like wearing it for yoga for this reason. Would love to find a great top (preferably a tank) that I don’t need to adjust or worry about with all the forward folds and down dogs. I knew lulu makes high neck bras but I don’t like the idea of something tight across my collarbone.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Hey all,\\n\\nI recently received ABC pants (size 32) from the store, and they fit well. However, I’ve heard from multiple friends that they lose their shape and stretch out/can become baggy after a couple of weeks-months of wear. Should I go back to the store and try a size 31 in anticipation of this? \\n\\nIf not, will lululemon replace these pants should they lose their shape? \\n\\nThanks!   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Hi lemonheads :D\\n\\nI was wondering if anyone has bought a pair of aligns that seems to be made from a thicker fabric than the usual Nulu. I think I saw a post a while ago where a few people thought their aligns were thicker, they were all dark red colour but I don't remember the exact name (not Garnet!)\\n\\nI recently purchased a pair of full length Aligns off Poshmark in the colour Graphite, but they also seem to be thicker, almost luon. I thought maybe they were WUs, but don't believe WUs come in that colour? I've attached a photo of the size dot (it looks real to me?), though I'm not sure how to read it or where I would check. Any advice or thoughts would be great! \\n\\nhttps://preview.redd.it/8zq5q04lf7841.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=acaa777817edaeacc94b1effefd42b6f3d4f8c18   \n",
       "\n",
       "   score  num_comments  \n",
       "0      1             7  \n",
       "1      0             6  \n",
       "2      3             4  \n",
       "3      1             6  \n",
       "4      3            11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine data\n",
    "\n",
    "examine_df('lulu dataframe', lulu_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5177d54-a7f7-4cdd-9b92-68cfc30587a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original dataframe\n",
    "\n",
    "og_lulu_df = lulu_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6023f0-b33e-459d-9c91-7bb61ba1f393",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad46964f-0897-4e64-9146-698a27e71aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset dataframe\n",
    "\n",
    "lulu_df = og_lulu_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73e6c995-fd90-4ae2-92fb-7abf829b6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop word list, lemmatizer, and regex\n",
    "\n",
    "# Stopwords\n",
    "\n",
    "custom_stop_words = [\n",
    "    # brand/boilerplate\n",
    "    \"lululemon\", \"lulu\", \"amp\", \"xx\", \"lol\",\n",
    "    \"like\", \"get\", \"got\", \"would\", \"anyone\", \"one\",\n",
    "\n",
    "    # deletion/removal artifacts\n",
    "    \"deleted\", \"remove\", \"removed\", \"removal\",\n",
    "    \"deleted_view\", \"removed_view\", \"view_poll\", \"poll_view\",\n",
    "    \"deleted_view_poll\", \"removed_view_poll\",\n",
    "    \"view\", \"poll\", \"results\", \"result\", \"vote\", \"votes\",\n",
    "    \"thread\", \"post\", \"posting\", \"posted\", \"comment\", \"comments\",\n",
    "\n",
    "    # generic low-information Reddit junk\n",
    "    \"http\", \"https\", \"www\", \"com\",\n",
    "    \"imgur\", \"jpg\", \"png\", \"gif\",\n",
    "    \"subreddit\", \"reddit\", \"mod\", \"mods\",\n",
    "    \"link\", \"links\",\n",
    "\n",
    "    # Scraped filler \n",
    "    \"user\", \"account\", \"profile\",\n",
    "    \"page\", \"site\", \"website\",\n",
    "    \"viewed\", \"views\", \"seen\"\n",
    "]\n",
    "\n",
    "base_stops = set(stopwords.words(\"english\"))\n",
    "base_stops -= {\"no\", \"nor\", \"not\", \"never\"}       # Keep negations\n",
    "\n",
    "stop_words = list(base_stops.union(custom_stop_words))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Precompile regex\n",
    "_link = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "_nonalpha = re.compile(r'[^a-z\\s]')\n",
    "_spaces = re.compile(r'\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8d60460-b4d5-4a6b-8630-198dca17822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text preprocessor\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess text before modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = _link.sub(\" \", text)         # Remove links\n",
    "    text = _nonalpha.sub(\" \", text)     # Keep only letters/spaces\n",
    "    tokens = []\n",
    "    for t in text.split():\n",
    "        if t in stop_words or len(t) < 3:\n",
    "            continue\n",
    "        t = lemmatizer.lemmatize(t)\n",
    "        tokens.append(t)\n",
    "    return _spaces.sub(\" \", \" \".join(tokens)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72a9bea5-1541-4a16-8e77-7f8beb48bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text preprocessing\n",
    "\n",
    "lulu_df[\"clean_text\"] = lulu_df[\"title\"].fillna(\"\") + \" \" + lulu_df[\"text\"].fillna(\"\")\n",
    "lulu_df[\"clean_text\"] = lulu_df[\"clean_text\"].apply(preprocess)\n",
    "\n",
    "# drop docs with <5 tokens to reduce noise\n",
    "lulu_df = lulu_df[lulu_df[\"clean_text\"].str.split().str.len() >= 5].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46d8da26-6a21-4296-9af5-c2328ec2da07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text data:\n",
      "\n",
      "\n",
      "Text sample 1:\n",
      "\n",
      "\n",
      "experienced pilling high rise stretch jogger thicker thigh obviously leg constantly rubbing together trying see worth exchanging adapt state jogger appreciate thought\n",
      "\n",
      "\n",
      "\n",
      "Text sample 2:\n",
      "\n",
      "\n",
      "group interview interview tommorw morning educator position local store credential following bachelor science nutrition science currently first year second semester master public health degree specilization dietetics program combine supervised practice hour become registered dietitican graduation shift supervisor last three year high volume cannabis dispensary see upwards people per day sale per day main question type question prepared know entry level retail position appreciate run expect never group interview thanks much\n",
      "\n",
      "\n",
      "\n",
      "Text sample 3:\n",
      "\n",
      "\n",
      "fianc fit looking fire description item featured photo rain chaser jacket black never lost keychains black grey grey sage colour align tights black swiftly top orange soda\n",
      "\n",
      "\n",
      "\n",
      "Text sample 4:\n",
      "\n",
      "\n",
      "gilroy outlet haul made stop gilroy outlet morning pleasantly surprised inventory mini haul item marked much ended paying\n",
      "\n",
      "\n",
      "\n",
      "Text sample 5:\n",
      "\n",
      "\n",
      "seawheeze drop seawheeze product expected drop\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check some examples\n",
    "\n",
    "get_text_samples(lulu_df, 'clean_text', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad115157-1c09-4811-a52a-727a98f748aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tokenized docs\n",
    "\n",
    "tokenized_docs = [doc.split() for doc in lulu_df[\"clean_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16994c30-0aa0-4fc8-9c39-54fdb700971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge bigrams\n",
    "\n",
    "def merge_bigrams(doc, bigram_set):\n",
    "    \n",
    "    \"\"\"\n",
    "    Merge bigrams\n",
    "    \"\"\"\n",
    "    \n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(doc):\n",
    "        if i < len(doc)-1 and (doc[i], doc[i+1]) in bigram_set:\n",
    "            merged.append(f\"{doc[i]}_{doc[i+1]}\")\n",
    "            i += 2\n",
    "        else:\n",
    "            merged.append(doc[i])\n",
    "            i += 1\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ae769bd-05cb-49af-be18-116d1fe5b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply bigram detector\n",
    "\n",
    "bigrams = [list(ngrams(doc, 2)) for doc in tokenized_docs]\n",
    "flat_bigrams = [bg for doc in bigrams for bg in doc]\n",
    "bigram_counts = Counter(flat_bigrams)\n",
    "common_bigrams = {bg for bg, count in bigram_counts.items() if count >= 10}\n",
    "\n",
    "tokenized_bigrams = [merge_bigrams(doc, common_bigrams) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de8ee4f7-834a-4222-ac7e-cfd09c469f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigram text back to dataframe\n",
    "\n",
    "lulu_df[\"clean_text_bigram\"] = [\" \".join(doc) for doc in tokenized_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "284f27ce-6c58-4933-9182-41c4dec0cede",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text data:\n",
      "\n",
      "\n",
      "Text sample 1:\n",
      "\n",
      "\n",
      "cloud wear_size regular_length\n",
      "\n",
      "\n",
      "\n",
      "Text sample 2:\n",
      "\n",
      "\n",
      "nomad perfect spring khaki scored wunder_train racer_back tank_nomad wmtm_love style weightlift find_fit much_comfortable align_tank thought nomad brown hued based pic pull khaki\n",
      "\n",
      "\n",
      "\n",
      "Text sample 3:\n",
      "\n",
      "\n",
      "lil reveal mesh mid_rise reveal mid_rise mesh posy design black_size cloud_bra black_size legging released mid_rise considered high\n",
      "\n",
      "\n",
      "\n",
      "Text sample 4:\n",
      "\n",
      "\n",
      "align_bra discontinued new_color drop regular_align bra_not shoulder neck know_discontinued sad neck cute low_cut liking not_even part big tittee committee\n",
      "\n",
      "\n",
      "\n",
      "Text sample 5:\n",
      "\n",
      "\n",
      "style amalfi coast lounge_wear cute_outfit recs beach sightseeing love_wunder train_fast free_legging feel athletic casual\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check some examples\n",
    "\n",
    "get_text_samples(lulu_df, 'clean_text_bigram', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65684850-18d9-4d51-95a3-4506a0cc21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of docs\n",
    "\n",
    "docs = lulu_df[\"clean_text_bigram\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63c400-9ffd-4ff1-9fac-c72130a4f429",
   "metadata": {},
   "source": [
    "## Modeling with BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b02223ad-717e-4bee-8fac-ffe2741ef9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline sentence embedding model\n",
    "\n",
    "EMB_NAME = \"all-MiniLM-L6-v2\"\n",
    "st_model = SentenceTransformer(EMB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e964b2f-3099-4d6a-821c-86bfc3974fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed48d27f392449b85cf825da5bbe9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1768 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "\n",
    "embs = st_model.encode(\n",
    "    docs,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True  # Normalizes vectors for cosine sim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e567c163-f008-4c86-b17e-e52fa0102513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure UMAP model\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors = 30,\n",
    "    n_components=5,\n",
    "    min_dist = 0.20,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d71dba56-5c5f-45d8-a1b8-8997edfdcf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DBSCAN model\n",
    "\n",
    "N = len(docs)\n",
    "\n",
    "min_cluster_size = max(200, math.floor(0.015 * N))  # ≈0.5% of corpus, at least 50\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples= 35,  # defaults to min_cluster_size; set e.g. 10–30 to merge a bit more\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    "    cluster_selection_epsilon = 0.00\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ab543562-e52b-4e20-9b93-29b957a9f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectorizer to keep unigrams and bigrams\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"(?u)\\b[\\w_]{3,}\\b\",\n",
    "    stop_words = None,\n",
    "        max_df = .9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "139b7a14-b8a6-4699-a2e3-7ea83a621576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BERTopic model\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=st_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language=\"english\",\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    top_n_words=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f3f8d524-2c5b-4c43-befb-81411c5ce901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:29:27,475 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-06 17:31:08,347 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-06 17:31:08,350 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-06 17:31:19,041 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-06 17:31:19,054 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-09-06 17:31:29,220 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full runtime: 2.03 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Fit BERTopic model\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs, embs)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end-start)/60\n",
    "\n",
    "print(f\"Full runtime: {runtime:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "63c58a7f-7a31-41d2-8d41-33bfae998da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered topics (excl. -1): 3\n",
      "Outlier docs (-1): 2958 / 56574 = 5.2%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>2958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>50478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count\n",
       "0     -1   2958\n",
       "1      0  50478\n",
       "2      1   1686\n",
       "3      2   1452"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine basic topic info\n",
    "\n",
    "# Topic table and basic stats\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.head(10)\n",
    "\n",
    "# Number of discovered topics (exclude -1 = outliers)\n",
    "n_topics = int((topic_info[\"Topic\"] != -1).sum())\n",
    "n_docs = len(docs)\n",
    "outlier_share = (topics.count(-1) / n_docs) if n_docs else 0.0\n",
    "\n",
    "print(f\"Discovered topics (excl. -1): {n_topics}\")\n",
    "print(f\"Outlier docs (-1): {topics.count(-1)} / {n_docs} = {outlier_share:.1%}\")\n",
    "\n",
    "# Topic size distribution\n",
    "topic_info[[\"Topic\", \"Count\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "bdaf1eac-83d6-4ae7-b041-11b4ac0ba378",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Topic 0 | size=50478\n",
      "\n",
      "Top words: high_rise, bra, jogger, energy_bra, long_sleeve, hoodie, gift_card, hotty_hot, dance_studio, align_short, sport_bra, swiftly_tech, ebb_street, restock, leg\n",
      "\n",
      "--- Ex1: usa new_item align_short diamond_dye vista_green saddle_brown align_short saddle_brown see_horizon tank_dune black swift_speed bra_black stroll_sundown short_cherry tint carnation_red true_navy black femme force crop_tank white_black trip_taker skirt carnation_red beech_wood black muscle_love crop_t...\n",
      "\n",
      "--- Ex2: new_item drop_super helpful_format thanks_advance item_name color_color linkhere energy_bra blue_black bra_energy bra_pink blossom cloud_longline bra_pink blossom top_ebb street_tank top_pink blossom cool_racerback short aero_blue align_tank waist_length pink_blossom love_tank soft_denim legging_ali...\n",
      "\n",
      "--- Ex3: drop_happy drop_day please_format item_name color_color energy_bra black_blue bra_energy bra_medium support_cup scream_green light free_serene bra_light support_cup poolside scream_green light free_wild cup poolside electric_turquoise cloud_bra light_support cup smoky_topaz cloud_ribbed longline_bra...\n",
      "================================================================================\n",
      "\n",
      "Topic 1 | size=1686\n",
      "\n",
      "Top words: new_crew, laptop, mini_shoulder, everyday_backpack, clean_line, compartment, gym_bag, key, diaper_bag, carrying, personal_item, new_crew backpack, fanny_pack, nano, mini_shoulder bag\n",
      "\n",
      "--- Ex1: happy experiment psa secure true_identity card_case clean_line belt_bag key loop excited never_used key wallet extra secure pictured clean_line belt_bag raw_linen rover true_identity card_case twilight_rose white_opal dark_lavender without_flash\n",
      "\n",
      "--- Ex2: mini_shoulder bag greu everywhere_belt bag_black blk blk spotted chadstone store australia morning new_colour mini_shoulder bag dropped morning australia also spotted everywhere_belt bag_black black_black store_today\n",
      "\n",
      "--- Ex3: new_parent tote airplane personal_item looking simple functional bag tote use personal_item flying looking_something functional side_pocket outside laptop sleeve carry pas sleeve set top luggage far tried bag away calpak seem functional even_though designed travel need came_across new_parent tote br...\n",
      "================================================================================\n",
      "\n",
      "Topic 2 | size=1452\n",
      "\n",
      "Top words: detergent, laundry, bleach, washer, scent, vinegar, air_dry, cold_water, hang_dry, drying, dyeing, oxiclean, laundry_detergent, dried, dish_soap\n",
      "\n",
      "--- Ex1: little_rant ended staining hoodie even using detergent blood stain_scuba hoodie used detergent dropped detergent spot didnt know whole day there blue spot hoodie wont come\n",
      "\n",
      "--- Ex2: much detergent washing clothes usually_wash stuff together use standard amount detergent hang_dry however cat really unhappy quarantine decided poop dirty laundry obviously put_washer immediately tide pod since disgusted wondering_much ruin clothes_also put_dryer small_amount time smell detergent af...\n",
      "\n",
      "--- Ex3: please_help short_coming unbearably itchy matter try various type residue several_month not_able pair_short come_wash wearable pair_aligns others wunder_train think often feel okay touch lack visible residue not slimy chalky wet wait line dried worn body know_whether okay never okay minute unbearabl...\n",
      "\n",
      "Non-noise topics: [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Check top words for each topic\n",
    "\n",
    "# Refresh topic_info from the current model\n",
    "topic_info = topic_model.get_topic_info().copy()\n",
    "\n",
    "# Get all non-noise topics\n",
    "topic_ids = topic_info.loc[topic_info[\"Topic\"] != -1, \"Topic\"].tolist()\n",
    "\n",
    "for t in topic_ids:\n",
    "    # safe size lookup\n",
    "    size = int(topic_info.loc[topic_info[\"Topic\"] == t, \"Count\"].iloc[0])\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTopic {t} | size={size}\")\n",
    "\n",
    "    # top words (guard if topic exists but empty)\n",
    "    words = topic_model.get_topic(t) or []\n",
    "    top_terms = \", \".join([w for w, _ in words[:15]]) if words else \"(no terms)\"\n",
    "    print(\"\\nTop words:\", top_terms)\n",
    "\n",
    "    # representative examples (guard empty)\n",
    "    reps = (topic_model.get_representative_docs(t) or [])[:3]\n",
    "    for i, doc in enumerate(reps, 1):\n",
    "        preview = doc[:300].replace(\"\\n\", \" \")\n",
    "        suffix = \"...\" if len(doc) > 300 else \"\"\n",
    "        print(f\"\\n--- Ex{i}: {preview}{suffix}\")\n",
    "\n",
    "print(\"\\nNon-noise topics:\", topic_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "264a6f3d-5ff1-4f79-b17f-ff9579d3b579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic diversity (for top 10 words for each topic): 1.000\n"
     ]
    }
   ],
   "source": [
    "# Check topic diversity\n",
    "\n",
    "TOP_N = 10\n",
    "words_per_topic = {\n",
    "    t: [w for w, _ in topic_model.get_topic(t)[:TOP_N]]\n",
    "    for t in topic_info[\"Topic\"].tolist() if t != -1\n",
    "}\n",
    "\n",
    "all_top_words = list(chain.from_iterable(words_per_topic.values()))\n",
    "diversity = len(set(all_top_words)) / max(1, len(all_top_words))\n",
    "print(f\"Topic diversity (for top {TOP_N} words for each topic): {diversity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b453792-a6b9-4101-94cb-4e887b78c3d5",
   "metadata": {},
   "source": [
    "## Subclustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "331ba7ca-5b3b-4d4d-b1d9-2e65981ea50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose biggest topic \n",
    "\n",
    "big_tid = 0 # index for big topic is 0\n",
    "\n",
    "mask = (np.array(topics) == big_tid)\n",
    "docs_big  = [d for d, m in zip(docs,  mask) if m]\n",
    "embs_big  = embs[mask]\n",
    "idx_big   = np.where(mask)[0]  # to map back later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2358033d-2157-4479-a068-fb158520f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new UMAP model\n",
    "\n",
    "umap_sub = UMAP(\n",
    "    n_neighbors=30,  \n",
    "    n_components=5,\n",
    "    min_dist=0.20,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "de484d0c-3702-44d9-8426-58bbfb4caaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new HDBSCAN model\n",
    "\n",
    "M = len(docs_big)\n",
    "\n",
    "hdb_sub = HDBSCAN(\n",
    "    min_cluster_size=max(150, int(0.015 * M)),\n",
    "    min_samples=45,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    "    cluster_selection_epsilon=0.02   # small tolerance for merges\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f8cc3536-3166-48d3-95ff-81d22b245c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up vectorizer\n",
    "\n",
    "vec_sub = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"(?u)\\b[\\w_]{3,}\\b\",\n",
    "    stop_words=None,\n",
    "    max_df=.95\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "16f951cd-1d50-4e2d-9672-a988b376da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new BERTopic model\n",
    "\n",
    "sub_model = BERTopic(\n",
    "    umap_model=umap_sub,\n",
    "    hdbscan_model=hdb_sub,\n",
    "    vectorizer_model=vec_sub,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "07d829c2-e12e-4967-9c91-92a98463dff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 19:15:04,339 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-06 19:16:21,122 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-06 19:16:21,126 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-06 19:16:29,284 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-06 19:16:29,306 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-09-06 19:16:38,612 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full runtime: 1.5750168601671855\n"
     ]
    }
   ],
   "source": [
    "# Fit submodel\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "sub_topics, sub_probs = sub_model.fit_transform(docs_big, embs_big)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start)/60\n",
    "\n",
    "print(f\"Full runtime: {runtime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "93f5aaf6-db07-494e-893b-9d28dbd601bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered subtopics (excl. -1): 2\n",
      "Outlier docs (-1): 16964 / 50478 = 33.6%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>16964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>31059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count\n",
       "0     -1  16964\n",
       "1      0  31059\n",
       "2      1   2455"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine subtopics\n",
    "\n",
    "# Point the inspectors at the SUB model/run\n",
    "MODEL  = sub_model\n",
    "DOCS   = docs_big\n",
    "TOPICS = list(sub_topics)   # ensure list so .count(-1) works\n",
    "\n",
    "# ---------- 1) Basic topic info ----------\n",
    "topic_info_sub = MODEL.get_topic_info().copy()\n",
    "\n",
    "# Number of discovered topics (exclude -1 = outliers)\n",
    "n_topics_sub   = int((topic_info_sub[\"Topic\"] != -1).sum())\n",
    "n_docs_sub     = len(DOCS)\n",
    "outlier_share  = (TOPICS.count(-1) / n_docs_sub) if n_docs_sub else 0.0\n",
    "\n",
    "print(f\"Discovered subtopics (excl. -1): {n_topics_sub}\")\n",
    "print(f\"Outlier docs (-1): {TOPICS.count(-1)} / {n_docs_sub} = {outlier_share:.1%}\")\n",
    "\n",
    "# Subtopic size distribution\n",
    "display(topic_info_sub[[\"Topic\", \"Count\"]].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c6e7c396-e58e-4cdd-b55b-a32eb39e7f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Topic 0 | size=31059\n",
      "\n",
      "Top words: refund, sweatcollective, employee, highneck, interview, courtrival, code, policy, scubahalf, priceadjustment\n",
      "\n",
      "--- Ex1: exchanging boxing_day sale_think saw_someone wanted_check wider audience policy allowing exchange boxing_day sale_item actually went_store today otf_jogger savannah changing_room local_store closed educator mentioned still_exchange sale_item different_size needed thru holiday return period not_sure ...\n",
      "\n",
      "--- Ex2: tracking stuck mercari sale_item shipped item_sold mercari using usps label day_ago tracking updated week say package_arrive later expected currently transit next facility happen long_wait filing claim feel_bad buyer probably really_want short\n",
      "\n",
      "--- Ex3: customer_support scam tldr ordered_item different address billing due helping brother health situation order_cancelled next_day tell unblock card unless submit credit_card statement info big_fan not_sure support longer recent_experience unfortunately brother bad_news serious health issue dropped eve...\n",
      "================================================================================\n",
      "\n",
      "Topic 1 | size=2455\n",
      "\n",
      "Top words: winterwarrior, insulated, definejacket sizinghelp, thanks definejacket, nuluhooded, definejacket nulualign, jackettried, jacketwant, knowjacket, jacketreview\n",
      "\n",
      "--- Ex1: insulated quilted jacket_tried jacket_since wmtm quilted product_line\n",
      "\n",
      "--- Ex2: insulated waterproof_jacket pack_jacket long_wanted buy_long jacket_warm wear_fall not_cold winter_day live vancouver something waterproof key wanted pack long_jacket saw_black totally stock mulling insulated waterproof_jacket also_long look warm detail warm fit_review say_size actually waterproof t...\n",
      "\n",
      "--- Ex3: rain_rebel rain_rebel insulated preference shopping_store accidentally_bought insulated rain_rebel instead_regular first_time newborn baby feeling_bit flustered rushing wanted_something wear mild winter_spring fall puffer not needing winter_coat vancouver area rain lot plan_use light walk stroller g...\n",
      "\n",
      "Non-noise topics: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Check top words for each subtopic\n",
    "\n",
    "# Refresh subtopic_info from the current model\n",
    "subtopic_info =sub_model.get_topic_info().copy()\n",
    "\n",
    "# Get all non-noise subtopics\n",
    "subtopic_ids = subtopic_info.loc[topic_info[\"Topic\"] != -1, \"Topic\"].tolist()\n",
    "\n",
    "for t in subtopic_ids:\n",
    "    # safe size lookup\n",
    "    size = int(subtopic_info.loc[subtopic_info[\"Topic\"] == t, \"Count\"].iloc[0])\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTopic {t} | size={size}\")\n",
    "\n",
    "    # top words (guard if subtopic exists but empty)\n",
    "    words = sub_model.get_topic(t) or []\n",
    "    top_terms = \", \".join([w for w, _ in words[:15]]) if words else \"(no terms)\"\n",
    "    print(\"\\nTop words:\", top_terms)\n",
    "\n",
    "    # representative examples (guard empty)\n",
    "    reps = (sub_model.get_representative_docs(t) or [])[:3]\n",
    "    for i, doc in enumerate(reps, 1):\n",
    "        preview = doc[:300].replace(\"\\n\", \" \")\n",
    "        suffix = \"...\" if len(doc) > 300 else \"\"\n",
    "        print(f\"\\n--- Ex{i}: {preview}{suffix}\")\n",
    "\n",
    "print(\"\\nNon-noise topics:\", subtopic_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b8f47-73ef-4dd0-b323-fa02aff965a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Grid Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7dabb5a5-5680-4f4c-87f5-bec0e489e5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_samples=25, eps=0.02 -> subtopics=10, noise=58.1%\n",
      "min_samples=30, eps=0.02 -> subtopics=6, noise=52.0%\n",
      "min_samples=35, eps=0.02 -> subtopics=9, noise=59.7%\n",
      "min_samples=40, eps=0.02 -> subtopics=7, noise=51.2%\n"
     ]
    }
   ],
   "source": [
    "# Grid search on epsilon and min sample says for HDBSCAN\n",
    "\n",
    "# reuse docs_big, embs_big, umap_sub, M (len(docs_big))\n",
    "min_cluster_size = max(200, int(0.015* M))  # ~1% of subset size\n",
    "\n",
    "ms_values  = [25,30,35,40]\n",
    "eps_values = [0.02]\n",
    "\n",
    "results = []\n",
    "\n",
    "for ms in ms_values:\n",
    "    for eps in eps_values:\n",
    "        hdb_sub = HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=ms,\n",
    "            metric=\"euclidean\",\n",
    "            cluster_selection_method=\"eom\",\n",
    "            prediction_data=True,\n",
    "            cluster_selection_epsilon=eps\n",
    "        )\n",
    "\n",
    "        vec_sub = CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            token_pattern=r\"(?u)\\b[\\w_]{3,}\\b\",\n",
    "            stop_words=None,\n",
    "            min_df=1,\n",
    "            max_df=1.0\n",
    "        )\n",
    "\n",
    "        sub_model = BERTopic(\n",
    "            umap_model=umap_sub,     # keep your existing UMAP config\n",
    "            hdbscan_model=hdb_sub,\n",
    "            vectorizer_model=vec_sub,\n",
    "            calculate_probabilities=True,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        sub_topics, sub_probs = sub_model.fit_transform(docs_big, embs_big)\n",
    "        sub_info = sub_model.get_topic_info()\n",
    "\n",
    "        n_subtopics = (sub_info.Topic != -1).sum()\n",
    "        noise_share = (np.array(sub_topics) == -1).mean()\n",
    "\n",
    "        results.append((ms, eps, n_subtopics, noise_share))\n",
    "        print(f\"min_samples={ms:2d}, eps={eps:.2f} \"\n",
    "              f\"-> subtopics={n_subtopics}, noise={noise_share:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76f613-57c0-4692-a259-bedbfea207c6",
   "metadata": {},
   "source": [
    "min_samples=20, eps=0.00 -> subtopics=12, noise=55.5%\n",
    "min_samples=20, eps=0.02 -> subtopics=12, noise=55.5%\n",
    "min_samples=20, eps=0.05 -> subtopics=12, noise=55.5%\n",
    "min_samples=25, eps=0.00 -> subtopics=11, noise=56.9% min_samples=25, eps=0.02 -> subtopics=10, noise=58.1%\n",
    "min_samples=30, eps=0.02 -> subtopics=6, noise=52.0%\n",
    "min_samples=35, eps=0.02 -> subtopics=9, noise=59.7%\n",
    "min_samples=40, eps=0.02 -> subtopics=7, noise=51.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2945dd40-7eb3-422e-b689-61000078925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for grid searches\n",
    "\n",
    "def run_once(mcs, ms, eps=0.02):\n",
    "    hdb = HDBSCAN(\n",
    "        min_cluster_size=mcs,\n",
    "        min_samples=ms,\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method=\"eom\",\n",
    "        prediction_data=True,\n",
    "        cluster_selection_epsilon=eps\n",
    "    )\n",
    "    vec = CountVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        token_pattern=r\"(?u)\\b[\\w_]{3,}\\b\",\n",
    "        stop_words=None,\n",
    "        min_df=1,\n",
    "        max_df=1.0\n",
    "    )\n",
    "    mdl = BERTopic(\n",
    "        umap_model=umap_sub,\n",
    "        hdbscan_model=hdb,\n",
    "        vectorizer_model=vec,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    t, _ = mdl.fit_transform(docs_big, embs_big)\n",
    "    info = mdl.get_topic_info()\n",
    "    k = int((info.Topic != -1).sum())\n",
    "    noise = float((np.array(t) == -1).mean())\n",
    "    print(f\"mcs={mcs:4d}, min_samples={ms:2d}, eps={eps:.2f}  →  subtopics={k:2d}, noise={noise:.1%}\")\n",
    "    return (k, noise, mdl, t, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8b47e8a9-fcd0-4982-a90e-c68082e1d293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcs= 504, min_samples=40, eps=0.02  →  subtopics=12, noise=56.1%\n",
      "mcs= 504, min_samples=45, eps=0.02  →  subtopics=13, noise=62.8%\n",
      "mcs= 504, min_samples=50, eps=0.02  →  subtopics=12, noise=55.4%\n",
      "mcs= 605, min_samples=40, eps=0.02  →  subtopics=10, noise=58.3%\n",
      "mcs= 605, min_samples=45, eps=0.02  →  subtopics=11, noise=65.0%\n",
      "mcs= 605, min_samples=50, eps=0.02  →  subtopics=10, noise=57.6%\n",
      "mcs= 757, min_samples=40, eps=0.02  →  subtopics= 7, noise=51.2%\n",
      "mcs= 757, min_samples=45, eps=0.02  →  subtopics= 2, noise=33.6%\n",
      "mcs= 757, min_samples=50, eps=0.02  →  subtopics=10, noise=57.6%\n",
      "mcs=1009, min_samples=40, eps=0.02  →  subtopics= 7, noise=51.2%\n",
      "mcs=1009, min_samples=45, eps=0.02  →  subtopics= 7, noise=58.7%\n",
      "mcs=1009, min_samples=50, eps=0.02  →  subtopics= 7, noise=46.9%\n",
      "\n",
      "Best by (min noise, then fewer clusters):\n",
      "mcs=757, min_samples=45, eps=0.02  →  subtopics=2, noise=33.6%\n"
     ]
    }
   ],
   "source": [
    "# Grid search 00\n",
    "\n",
    "M = len(docs_big)\n",
    "\n",
    "# SMALL grid (aiming to reduce both cluster count and noise)\n",
    "mcs_fracs = [0.010, 0.012, 0.015, 0.02]      # ≈ 1.0%, 1.2%, 1.5% of subset\n",
    "ms_values = [40, 45, 50]           # around your previous best\n",
    "epsilon   = 0.02                       # mild merge tolerance\n",
    "\n",
    "best = {\"noise\": 1.0, \"k\": 10**9, \"cfg\": None, \"model\": None, \"topics\": None, \"info\": None}\n",
    "\n",
    "for frac in mcs_fracs:\n",
    "    mcs = max(120, int(frac * M))\n",
    "    for ms in ms_values:\n",
    "        k, noise, mdl, t, info = run_once(mcs, ms, epsilon)\n",
    "        # choose best by: lowest noise, then fewer clusters\n",
    "        if (noise < best[\"noise\"]) or (noise == best[\"noise\"] and k < best[\"k\"]):\n",
    "            best.update({\"noise\": noise, \"k\": k, \"cfg\": (mcs, ms, epsilon),\n",
    "                         \"model\": mdl, \"topics\": t, \"info\": info})\n",
    "\n",
    "print(\"\\nBest by (min noise, then fewer clusters):\")\n",
    "print(f\"mcs={best['cfg'][0]}, min_samples={best['cfg'][1]}, eps={best['cfg'][2]:.2f}  \"\n",
    "      f\"→  subtopics={best['k']}, noise={best['noise']:.1%}\")\n",
    "\n",
    "# Keep best handy if you want to inspect words/examples next:\n",
    "best_sub_model  = best[\"model\"]\n",
    "best_sub_topics = best[\"topics\"]\n",
    "best_sub_info   = best[\"info\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebe2aa-fd63-4626-99dd-1b4f620c52f3",
   "metadata": {},
   "source": [
    "mcs= 504, min_samples=40, eps=0.02  →  subtopics=12, noise=56.1%\n",
    "mcs= 504, min_samples=45, eps=0.02  →  subtopics=13, noise=62.8%\n",
    "mcs= 504, min_samples=50, eps=0.02  →  subtopics=12, noise=55.4%\n",
    "mcs= 605, min_samples=40, eps=0.02  →  subtopics=10, noise=58.3%\n",
    "mcs= 605, min_samples=45, eps=0.02  →  subtopics=11, noise=65.0%\n",
    "mcs= 605, min_samples=50, eps=0.02  →  subtopics=10, noise=57.6%\n",
    "mcs= 757, min_samples=40, eps=0.02  →  subtopics= 7, noise=51.2%\n",
    "mcs= 757, min_samples=45, eps=0.02  →  subtopics= 2, noise=33.6%\n",
    "mcs= 757, min_samples=50, eps=0.02  →  subtopics=10, noise=57.6%\n",
    "mcs=1009, min_samples=40, eps=0.02  →  subtopics= 7, noise=51.2%\n",
    "mcs=1009, min_samples=45, eps=0.02  →  subtopics= 7, noise=58.7%\n",
    "mcs=1009, min_samples=50, eps=0.02  →  subtopics= 7, noise=46.9%\n",
    "\n",
    "Best by (min noise, then fewer clusters):\n",
    "mcs=757, min_samples=45, eps=0.02  →  subtopics=2, noise=33.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e5e8e-0060-47e8-b8df-e8c2eb4f0304",
   "metadata": {},
   "source": [
    "## Topic Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "88292876-71eb-464e-b5b8-d82015cb7c83",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[135]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Overall map of topics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\lulu_bert_env\\Lib\\site-packages\\bertopic\\_bertopic.py:2424\u001b[39m, in \u001b[36mBERTopic.visualize_topics\u001b[39m\u001b[34m(self, topics, top_n_topics, use_ctfidf, custom_labels, title, width, height)\u001b[39m\n\u001b[32m   2391\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Visualize topics, their sizes, and their corresponding words.\u001b[39;00m\n\u001b[32m   2392\u001b[39m \n\u001b[32m   2393\u001b[39m \u001b[33;03mThis visualization is highly inspired by LDAvis, a great visualization\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2421\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m   2422\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2423\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mplotting\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_topics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2425\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_n_topics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_n_topics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2428\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_ctfidf\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_ctfidf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2433\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\lulu_bert_env\\Lib\\site-packages\\bertopic\\plotting\\_topics.py:74\u001b[39m, in \u001b[36mvisualize_topics\u001b[39m\u001b[34m(topic_model, topics, top_n_topics, use_ctfidf, custom_labels, title, width, height)\u001b[39m\n\u001b[32m     72\u001b[39m     words = [topic_model.custom_labels_[topic + topic_model._outliers] \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topic_list]\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     words = \u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m | \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtopic_list\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Embed c-TF-IDF into 2D\u001b[39;00m\n\u001b[32m     77\u001b[39m all_topics = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(topic_model.get_topics().keys()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\lulu_bert_env\\Lib\\site-packages\\bertopic\\plotting\\_topics.py:74\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     72\u001b[39m     words = [topic_model.custom_labels_[topic + topic_model._outliers] \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topic_list]\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     words = [\u001b[33m\"\u001b[39m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m.join([word[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m[:\u001b[32m5\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topic_list]\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Embed c-TF-IDF into 2D\u001b[39;00m\n\u001b[32m     77\u001b[39m all_topics = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(topic_model.get_topics().keys()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\lulu_bert_env\\Lib\\site-packages\\bertopic\\_bertopic.py:1612\u001b[39m, in \u001b[36mBERTopic.get_topic\u001b[39m\u001b[34m(self, topic, full)\u001b[39m\n\u001b[32m   1596\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return top n words for a specific topic and their c-TF-IDF scores.\u001b[39;00m\n\u001b[32m   1597\u001b[39m \n\u001b[32m   1598\u001b[39m \u001b[33;03mArguments:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1609\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m   1610\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1611\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtopic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtopic_representations_\u001b[49m:\n\u001b[32m   1613\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m full:\n\u001b[32m   1614\u001b[39m         representations = {\u001b[33m\"\u001b[39m\u001b[33mMain\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.topic_representations_[topic]}\n",
      "\u001b[31mTypeError\u001b[39m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "# Overall map of topics\n",
    "\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "66c4da09-3e1b-4b0c-a40e-f18ccf06adfe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualize top words per topic (bar charts)\n",
    "\n",
    "topic_model.visualize_barchart(top_n_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "334e3538-8eb0-4793-90e1-6ad1a851f921",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualizie topic similarity heatmap\n",
    "\n",
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82a33566-3d9b-448f-b28a-fb2c01b69d12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualize hierarchy (dendrogram)\n",
    "\n",
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b428b22-6833-4715-8b63-d80264f40e35",
   "metadata": {},
   "source": [
    "## Save topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12d0bbc-62e5-45d8-8bfa-ff9335099157",
   "metadata": {},
   "source": [
    "## Random extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e2bf9914-a235-4026-89be-e3323093ca03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_dist is set to: 0.185\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_bertopic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[228]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmin_dist is set to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_dist\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m topic_model, topics, probs = \u001b[43mrun_bertopic\u001b[49m(docs, embs, st_model, min_dist = min_dist)\n\u001b[32m     10\u001b[39m end = time.time()\n\u001b[32m     12\u001b[39m runtime = (end-start)/\u001b[32m60\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'run_bertopic' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit BERTopic model\n",
    "\n",
    "for min_dist in [0.185,0.19,0.195]:\n",
    "\n",
    "    print(f\"min_dist is set to: {min_dist}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    topic_model, topics, probs = run_bertopic(docs, embs, st_model, min_dist = min_dist)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    runtime = (end-start)/60\n",
    "    \n",
    "    print(f\"Full runtime: {runtime:.2f} minutes.\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c7234-9f83-43f7-9309-8015352503c9",
   "metadata": {},
   "source": [
    "2025-09-06 21:05:09,931 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
    "min_dist is set to: 0.185\n",
    "2025-09-06 21:06:58,259 - BERTopic - Dimensionality - Completed ✓\n",
    "2025-09-06 21:06:58,262 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
    "2025-09-06 21:07:12,363 - BERTopic - Cluster - Completed ✓\n",
    "2025-09-06 21:07:12,388 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
    "2025-09-06 21:07:23,953 - BERTopic - Representation - Completed ✓\n",
    "Discovered topics (excl. -1): 2\n",
    "Outlier docs (-1): 8600 / 56574 = 15.2%\n",
    "Topic\tCount\n",
    "0\t-1\t8600\n",
    "1\t0\t46303\n",
    "2\t1\t1671\n",
    "Full runtime: 2.24 minutes.\n",
    "\n",
    "\n",
    "\n",
    "min_dist is set to: 0.19\n",
    "2025-09-06 21:07:24,387 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
    "2025-09-06 21:09:08,456 - BERTopic - Dimensionality - Completed ✓\n",
    "2025-09-06 21:09:08,460 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
    "2025-09-06 21:09:19,490 - BERTopic - Cluster - Completed ✓\n",
    "2025-09-06 21:09:19,505 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
    "2025-09-06 21:09:30,357 - BERTopic - Representation - Completed ✓\n",
    "Discovered topics (excl. -1): 3\n",
    "Outlier docs (-1): 12082 / 56574 = 21.4%\n",
    "Topic\tCount\n",
    "0\t-1\t12082\n",
    "1\t0\t41480\n",
    "2\t1\t1704\n",
    "3\t2\t1308\n",
    "Full runtime: 2.11 minutes.\n",
    "\n",
    "\n",
    "\n",
    "min_dist is set to: 0.195\n",
    "2025-09-06 21:09:30,877 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
    "2025-09-06 21:11:09,888 - BERTopic - Dimensionality - Completed ✓\n",
    "2025-09-06 21:11:09,908 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
    "2025-09-06 21:11:22,767 - BERTopic - Cluster - Completed ✓\n",
    "2025-09-06 21:11:22,781 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
    "2025-09-06 21:11:34,157 - BERTopic - Representation - Completed ✓\n",
    "Discovered topics (excl. -1): 2\n",
    "Outlier docs (-1): 3776 / 56574 = 6.7%\n",
    "Topic\tCount\n",
    "0\t-1\t3776\n",
    "1\t0\t51092\n",
    "2\t1\t1706\n",
    "Full runtime: 2.06 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac97105-3fec-482f-bb41-61dfe8db7b5a",
   "metadata": {},
   "source": [
    "Discovered topics (excl. -1): 2\n",
    "Outlier docs (-1): 9566 / 56574 = 16.9%\n",
    "Topic\tCount\n",
    "0\t-1\t9566\n",
    "1\t0\t45320\n",
    "2\t1\t1688\n",
    "Number of neighbors is set to 5\n",
    "Full runtime: 1.03 minutes.\n",
    "\n",
    "\n",
    "\n",
    "Discovered topics (excl. -1): 2\n",
    "Outlier docs (-1): 2905 / 56574 = 5.1%\n",
    "Topic\tCount\n",
    "0\t-1\t2905\n",
    "1\t0\t51880\n",
    "2\t1\t1789\n",
    "Number of neighbors is set to 15\n",
    "Full runtime: 1.42 minutes.\n",
    "\n",
    "\n",
    "\n",
    "Discovered topics (excl. -1): 3\n",
    "Outlier docs (-1): 7356 / 56574 = 13.0%\n",
    "Topic\tCount\n",
    "0\t-1\t7356\n",
    "1\t0\t46053\n",
    "2\t1\t1744\n",
    "3\t2\t1421\n",
    "Number of neighbors is set to 18\n",
    "Full runtime: 1.54 minutes.\n",
    "\n",
    "\n",
    "\n",
    "Discovered topics (excl. -1): 2\n",
    "Outlier docs (-1): 4988 / 56574 = 8.8%\n",
    "Topic\tCount\n",
    "0\t-1\t4988\n",
    "1\t0\t49844\n",
    "2\t1\t1742\n",
    "Number of neighbors is set to 20\n",
    "Full runtime: 1.61 minutes.\n",
    "\n",
    "\n",
    "\n",
    "Discovered topics (excl. -1): 10\n",
    "Outlier docs (-1): 31751 / 56574 = 56.1%\n",
    "Topic\tCount\n",
    "0\t-1\t31751\n",
    "1\t0\t6610\n",
    "2\t1\t4385\n",
    "3\t2\t2491\n",
    "4\t3\t2204\n",
    "5\t4\t1907\n",
    "6\t5\t1744\n",
    "7\t6\t1697\n",
    "8\t7\t1672\n",
    "9\t8\t1180\n",
    "10\t9\t933\n",
    "Number of neighbors is set to 25\n",
    "Full runtime: 1.78 minutes.\n",
    "\n",
    "\n",
    "\n",
    "Discovered topics (excl. -1): 11\n",
    "Outlier docs (-1): 32210 / 56574 = 56.9%\n",
    "Topic\tCount\n",
    "0\t-1\t32210\n",
    "1\t0\t4203\n",
    "2\t1\t3162\n",
    "3\t2\t2414\n",
    "4\t3\t2397\n",
    "5\t4\t2395\n",
    "6\t5\t2184\n",
    "7\t6\t1922\n",
    "8\t7\t1686\n",
    "9\t8\t1600\n",
    "10\t9\t1289\n",
    "11\t10\t1112\n",
    "Number of neighbors is set to 27\n",
    "Full runtime: 1.84 minutes.\n",
    "\n",
    "\n",
    "\n",
    "Discovered topics (excl. -1): 2\n",
    "Outlier docs (-1): 5402 / 56574 = 9.5%\n",
    "Topic\tCount\n",
    "0\t-1\t5402\n",
    "1\t0\t49465\n",
    "2\t1\t1707\n",
    "Number of neighbors is set to 35\n",
    "Full runtime: 2.08 minutes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lulu_bert_env)",
   "language": "python",
   "name": "lulu_bert_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
