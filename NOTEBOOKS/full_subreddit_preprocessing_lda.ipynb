{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f45f844-7678-47d9-81c9-492d8571f422",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac48e06-1b3a-4dd8-ba25-f8cc9daee548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# General\n",
    "import json\n",
    "import zstandard as zstd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import Phrases, CoherenceModel\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa28980-b7fb-4c70-92dd-ff0903377914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set user's data path\n",
    "\n",
    "PATH = f\"C:/Users/emshe/Desktop/BRAINSTATION/LULULEMON/DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef3de349-221d-42dd-92ac-bf8a5066d110",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Download NLTK files (run once)\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7c212-fa67-40e1-872f-bfd63fe1382a",
   "metadata": {},
   "source": [
    "## Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7ec57d-8299-43dd-9343-252d47d0bae0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "\n",
    "def clean_text(s: str | None) -> str | None:\n",
    "    \n",
    "    '''\n",
    "    Clean string by substituting spaces for problematic characters\n",
    "    '''\n",
    "    \n",
    "    if s is None:\n",
    "        return None\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b388f449-e518-4bf3-8680-e6024a8e3608",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to get datetime from UTC timestamp\n",
    "\n",
    "def dt_from_epoch(ts: Optional[int]):\n",
    "\n",
    "    \"\"\"\n",
    "    Convert timestamp to pd.datetime format\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if ts is None:\n",
    "        return None\n",
    "    return pd.to_datetime(ts, unit=\"s\", utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c31afab8-4692-43da-87bc-0723484c41b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to examine dataframes\n",
    "\n",
    "def examine_df(name,df,\n",
    "               include_stats = True,\n",
    "               include_sample = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check basic info about a dataframe df\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n\\nNumber of records in the {name} is: {len(df)}\\n\")\n",
    "    print(f\"\\nNumber of features in the {name} is: {len(df.columns)}\\n\")\n",
    "    print(f\"The columns in the {name} are: {df.columns}\\n\")\n",
    "    print(f\"\\n Other info about {name}:\\n\")\n",
    "    display(df.info())\n",
    "    if include_stats == True:\n",
    "        print(f'\\n Basic statistical info about {name}:\\n')\n",
    "        display(df.describe())\n",
    "    if include_sample == True:\n",
    "        print(f\"\\n\\nSample of records in the {name}:\")\n",
    "        display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94e964fb-cd5c-4f31-a3de-b34833a66d2d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to get sample from text column\n",
    "\n",
    "def get_text_samples(df: pd.DataFrame, text_col: str, n: int) -> None:\n",
    "\n",
    "    '''\n",
    "    Print n samples from a text column in a dataframe\n",
    "    '''\n",
    "\n",
    "    # Ensure pandas doesn't truncate text\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    # Sample and print 5 full negative reviews\n",
    "    print(\"Sample text data:\\n\\n\")\n",
    "    sample = df[text_col].sample(n)\n",
    "    for i, description in enumerate(sample, 1):\n",
    "        print(f\"Text sample {i}:\\n\\n\\n{description}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f37be0d6-5f22-4ed3-87e1-b4a69677e0b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function for categorical bar graph\n",
    "\n",
    "def bar_graph(df: pd.DataFrame, col: str) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Generate bar graph for categorical column in a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in dataframe\")\n",
    "\n",
    "    counts = posts_df[col].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(f\"Distribution of {col.title()}\")\n",
    "    plt.xlabel(f\"{col.title()}\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc8ae594-562e-49e5-a6e3-0f7c126993ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define function to plot histogram for numeric columns\n",
    "\n",
    "def histogram(df: pd.DataFrame, \n",
    "             col: str,\n",
    "            bins: int = 30,\n",
    "             log: bool = False) -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a histogram for a numeric column in a dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in dataframe\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    df[col].dropna().hist(bins=bins, edgecolor=\"black\", log=log)\n",
    "    plt.title(f\"Histogram of {col.title()}\")\n",
    "    plt.xlabel(col.title())\n",
    "    plt.ylabel(\"Log(Frequency)\" if log else \"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a314a84b-a9ef-4b8c-b3f7-6fc4a06bb7b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to load ndjson\n",
    "\n",
    "def load_plain_ndjson(path: str, limit: Optional[int] = None) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Load a plain-text NDJSON file line by line into a DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            rows.append({\n",
    "                \"post_id\": obj.get(\"id\"),\n",
    "                \"timestamp\": dt_from_epoch(obj.get(\"created_utc\")),\n",
    "                \"author\": obj.get(\"author\"),\n",
    "                \"title\": obj.get(\"title\"),\n",
    "                \"text\": obj.get(\"selftext\"),\n",
    "                \"score\": obj.get(\"score\"),\n",
    "                \"num_comments\": obj.get(\"num_comments\"),\n",
    "                \"permalink\": obj.get(\"permalink\"),\n",
    "                \"subreddit\": obj.get(\"subreddit\"),\n",
    "            })\n",
    "\n",
    "            if limit and i >= limit:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc6245-b542-4dd5-8e0d-0f05a02f765f",
   "metadata": {},
   "source": [
    "## Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efffc6ad-ecdd-4a46-8730-a7ea402b83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean data\n",
    "\n",
    "lulu_df = pd.read_parquet(f\"{PATH}/lululemon_submissions_clean.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0925c71e-8b94-41d1-a28b-f6df482c8149",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of records in the lulu dataframe is: 57984\n",
      "\n",
      "\n",
      "Number of features in the lulu dataframe is: 6\n",
      "\n",
      "The columns in the lulu dataframe are: Index(['post_id', 'timestamp', 'title', 'text', 'score', 'num_comments'], dtype='object')\n",
      "\n",
      "\n",
      " Other info about lulu dataframe:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57984 entries, 0 to 57983\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype              \n",
      "---  ------        --------------  -----              \n",
      " 0   post_id       57984 non-null  object             \n",
      " 1   timestamp     57984 non-null  datetime64[ns, UTC]\n",
      " 2   title         57984 non-null  object             \n",
      " 3   text          57984 non-null  object             \n",
      " 4   score         57984 non-null  int64              \n",
      " 5   num_comments  57984 non-null  int64              \n",
      "dtypes: datetime64[ns, UTC](1), int64(2), object(3)\n",
      "memory usage: 2.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Basic statistical info about lulu dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>57984.000000</td>\n",
       "      <td>57984.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.446071</td>\n",
       "      <td>14.705126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>87.240166</td>\n",
       "      <td>40.279924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11864.000000</td>\n",
       "      <td>1987.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score  num_comments\n",
       "count  57984.000000  57984.000000\n",
       "mean      23.446071     14.705126\n",
       "std       87.240166     40.279924\n",
       "min        0.000000      0.000000\n",
       "25%        1.000000      2.000000\n",
       "50%        3.000000      6.000000\n",
       "75%       13.000000     13.000000\n",
       "max    11864.000000   1987.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sample of records in the lulu dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eielly</td>\n",
       "      <td>2020-01-01 05:33:25+00:00</td>\n",
       "      <td>Monthly Sales Post- January</td>\n",
       "      <td>FS: Aligns sz 4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eii06s</td>\n",
       "      <td>2020-01-01 12:46:35+00:00</td>\n",
       "      <td>Major problem falling down leggings?</td>\n",
       "      <td>Hello, over the last year I have been ordering...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eijtca</td>\n",
       "      <td>2020-01-01 16:00:56+00:00</td>\n",
       "      <td>Tops for yoga</td>\n",
       "      <td>I have a couple swiftly tech racerbacks for ho...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eikiew</td>\n",
       "      <td>2020-01-01 16:59:27+00:00</td>\n",
       "      <td>ABC Pants - Sizing</td>\n",
       "      <td>Hey all,\\n\\nI recently received ABC pants (siz...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eil4bb</td>\n",
       "      <td>2020-01-01 17:46:20+00:00</td>\n",
       "      <td>Certain Aligns colours with thicker fabric?</td>\n",
       "      <td>Hi lemonheads :D\\n\\nI was wondering if anyone ...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                 timestamp  \\\n",
       "0  eielly 2020-01-01 05:33:25+00:00   \n",
       "1  eii06s 2020-01-01 12:46:35+00:00   \n",
       "2  eijtca 2020-01-01 16:00:56+00:00   \n",
       "3  eikiew 2020-01-01 16:59:27+00:00   \n",
       "4  eil4bb 2020-01-01 17:46:20+00:00   \n",
       "\n",
       "                                         title  \\\n",
       "0                  Monthly Sales Post- January   \n",
       "1         Major problem falling down leggings?   \n",
       "2                                Tops for yoga   \n",
       "3                           ABC Pants - Sizing   \n",
       "4  Certain Aligns colours with thicker fabric?   \n",
       "\n",
       "                                                text  score  num_comments  \n",
       "0                                    FS: Aligns sz 4      1             7  \n",
       "1  Hello, over the last year I have been ordering...      0             6  \n",
       "2  I have a couple swiftly tech racerbacks for ho...      3             4  \n",
       "3  Hey all,\\n\\nI recently received ABC pants (siz...      1             6  \n",
       "4  Hi lemonheads :D\\n\\nI was wondering if anyone ...      3            11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine data\n",
    "\n",
    "examine_df('lulu dataframe', lulu_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5177d54-a7f7-4cdd-9b92-68cfc30587a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original dataframe\n",
    "\n",
    "og_lulu_df = lulu_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6023f0-b33e-459d-9c91-7bb61ba1f393",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e6c995-fd90-4ae2-92fb-7abf829b6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop word list, lemmatizer, and regex\n",
    "\n",
    "# Stopwords\n",
    "custom_stop_words = ['lululemon', 'lulu','amp','xx','lol', \"like\", \"get\", \"got\", \"would\", \"anyone\", \"one\"]  # Add any brand boilerplate tokens here\n",
    "\n",
    "base_stops = set(stopwords.words(\"english\"))\n",
    "base_stops -= {\"no\", \"nor\", \"not\", \"never\"}       # Keep negations\n",
    "stop_words = base_stops.union(custom_stop_words)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Precompile regex\n",
    "_link = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "_nonalpha = re.compile(r'[^a-z\\s]')\n",
    "_spaces = re.compile(r'\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8d60460-b4d5-4a6b-8630-198dca17822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text preprocessor\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess text before modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = _link.sub(\" \", text)         # Remove links\n",
    "    text = _nonalpha.sub(\" \", text)     # Keep only letters/spaces\n",
    "    tokens = []\n",
    "    for t in text.split():\n",
    "        if t in stop_words or len(t) < 3:\n",
    "            continue\n",
    "        t = lemmatizer.lemmatize(t)\n",
    "        tokens.append(t)\n",
    "    return _spaces.sub(\" \", \" \".join(tokens)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72a9bea5-1541-4a16-8e77-7f8beb48bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text preprocessing\n",
    "\n",
    "lulu_df[\"clean_text\"] = lulu_df[\"title\"].fillna(\"\") + \" \" + lulu_df[\"text\"].fillna(\"\")\n",
    "lulu_df[\"clean_text\"] = lulu_df[\"clean_text\"].apply(preprocess)\n",
    "\n",
    "# drop docs with <5 tokens to reduce noise\n",
    "lulu_df = lulu_df[lulu_df[\"clean_text\"].str.split().str.len() >= 5].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46d8da26-6a21-4296-9af5-c2328ec2da07",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text data:\n",
      "\n",
      "\n",
      "Text sample 1:\n",
      "\n",
      "\n",
      "hemming wunder train short fix sausage leg issue know hem bring store hemming tights top read leg opening squeeze even ppl skinny leg wondering hemming cut higher wider point solve issue tried checked pair sale\n",
      "\n",
      "\n",
      "\n",
      "Text sample 2:\n",
      "\n",
      "\n",
      "exchange policy lemon change policy exchange gave hard time exchangeinf pair pant anyway made show receipt\n",
      "\n",
      "\n",
      "\n",
      "Text sample 3:\n",
      "\n",
      "\n",
      "hat golf fianc birthday coming mentioned needing new hat golf currently lll suggestion thanks\n",
      "\n",
      "\n",
      "\n",
      "Text sample 4:\n",
      "\n",
      "\n",
      "define hooded jacket nulu question define hooded nulu jacket length wise regular define jacket found size regular define size cropped define\n",
      "\n",
      "\n",
      "\n",
      "Text sample 5:\n",
      "\n",
      "\n",
      "otf crop otf short weird sizing otf crop short luxtreme arrived today loved style colour fit super baggy front area looked camel toe extra baggy material front crotch area size style normally bottom may order instead\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check some examples\n",
    "\n",
    "get_text_samples(lulu_df, 'clean_text', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b8fd9-8b5f-4853-aa8a-75ff43e17627",
   "metadata": {},
   "source": [
    "## Build Dictionary and Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad115157-1c09-4811-a52a-727a98f748aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tokenized docs\n",
    "\n",
    "tokenized_docs = [doc.split() for doc in lulu_df[\"clean_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ae769bd-05cb-49af-be18-116d1fe5b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build bigram detector\n",
    "\n",
    "bigram = Phrases(tokenized_docs, min_count=10, threshold=10)  \n",
    "bigram_mod = Phraser(bigram)\n",
    "\n",
    "# Apply the trained model\n",
    "tokenized_bigrams = [bigram_mod[doc] for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de8ee4f7-834a-4222-ac7e-cfd09c469f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigram text back to dataframe\n",
    "\n",
    "lulu_df[\"clean_text_bigram\"] = [\" \".join(doc) for doc in tokenized_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "284f27ce-6c58-4933-9182-41c4dec0cede",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text data:\n",
      "\n",
      "\n",
      "Text sample 1:\n",
      "\n",
      "\n",
      "desert_teal come view_poll\n",
      "\n",
      "\n",
      "\n",
      "Text sample 2:\n",
      "\n",
      "\n",
      "cropped full_length deleted_view poll\n",
      "\n",
      "\n",
      "\n",
      "Text sample 3:\n",
      "\n",
      "\n",
      "wear date_brown fell_love date_brown wunder_train store idea color pair dark blue navy night_sea black horrible color coordinating unless easy color color pair date_brown\n",
      "\n",
      "\n",
      "\n",
      "Text sample 4:\n",
      "\n",
      "\n",
      "align keyhole strappy racerback mesh_panel tight obsessed item\n",
      "\n",
      "\n",
      "\n",
      "Text sample 5:\n",
      "\n",
      "\n",
      "lay employee following growth slowdown fear weighing stock two consecutive year increasing annual sale management see sale growing current fiscal year largely thanks challenging consumer environment shift consumer behavior late navigating slower start year market ceo calvin mcdonald said march earnings call\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check some examples\n",
    "\n",
    "get_text_samples(lulu_df, 'clean_text_bigram', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1daeb5af-c993-4ea7-9f85-c9085c83ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tokenized docs with bigrams\n",
    "\n",
    "tokenized_docs = [doc.split() for doc in lulu_df[\"clean_text_bigram\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bae351e1-2329-405e-8aa5-dd0a8813d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary mapping\n",
    "\n",
    "dictionary = Dictionary(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af49e018-bb9d-4707-8196-5d6e62f0389e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 4754 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# Filter extremes to keep vocab manageable\n",
    "\n",
    "dictionary.filter_extremes(no_below=30, no_above=0.5)  \n",
    "\n",
    "# no_below = n keep words that appear in at least n docs (too rare)\n",
    "# no_above = m → drop words that appear in > m% of docs (too common)\n",
    "\n",
    "print(f\"Dictionary size: {len(dictionary)} unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d7b2d56-d560-4279-9387-32b0528926ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert docs to Bag-of-Words using the dictionary\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d885b3d5-5bfc-4a43-8a33-265a7af69aed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First doc BoW: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]\n",
      "Decoded: [('aligns', 1), ('january', 1), ('monthly', 1), ('post', 1), ('sale', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first document’s representation\n",
    "\n",
    "print(\"First doc BoW:\", corpus[0])\n",
    "print(\"Decoded:\", [(dictionary[id], freq) for id, freq in corpus[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533da0c-74a4-40f3-9d09-7deb2b3c4311",
   "metadata": {},
   "source": [
    "## Modeling with Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa26d65c-139f-4370-aa99-f79501777e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test coherence for different numbers of topics\n",
    "\n",
    "def coherence_test(dictionary, corpus, texts, limit, start=5, step=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Define function to evaluate LDA model with coherence metric\n",
    "    \"\"\"\n",
    "\n",
    "    full_start = time.time()\n",
    "    \n",
    "    results = []\n",
    "    for num_topics in range(start, limit+1, step):\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        lda = LdaModel(corpus=corpus,\n",
    "                       id2word=dictionary,\n",
    "                       num_topics=num_topics,\n",
    "                       passes=10,\n",
    "                       random_state=42)\n",
    "        coherence_model = CoherenceModel(model=lda, texts=texts,\n",
    "                                         dictionary=dictionary, coherence=\"c_v\")\n",
    "        \n",
    "        coherence = coherence_model.get_coherence()\n",
    "        \n",
    "        results.append((num_topics, coherence))\n",
    "\n",
    "        end = time.time()\n",
    "        runtime = (end - start)/60\n",
    "        \n",
    "        print(f\"Topics: {num_topics}, Coherence: {coherence:.4f}, Runtime: {runtime:.2f} minutes\\n\\n\")\n",
    "    \n",
    "    full_end = time.time()\n",
    "    runtime = (full_end - full_start)/60\n",
    "\n",
    "    print(f\"\\nFull coherence test completed in {runtime:.2f} minutes.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e8ba32f-e74a-487e-b46c-1a91992502df",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 5, Coherence: 0.5289, Runtime: 3.27 minutes\n",
      "\n",
      "\n",
      "Topics: 6, Coherence: 0.5501, Runtime: 3.39 minutes\n",
      "\n",
      "\n",
      "Topics: 7, Coherence: 0.5097, Runtime: 3.32 minutes\n",
      "\n",
      "\n",
      "Topics: 8, Coherence: 0.5351, Runtime: 3.36 minutes\n",
      "\n",
      "\n",
      "Topics: 9, Coherence: 0.5346, Runtime: 3.39 minutes\n",
      "\n",
      "\n",
      "Topics: 10, Coherence: 0.6016, Runtime: 3.30 minutes\n",
      "\n",
      "\n",
      "Topics: 11, Coherence: 0.5253, Runtime: 3.19 minutes\n",
      "\n",
      "\n",
      "Topics: 12, Coherence: 0.5793, Runtime: 3.20 minutes\n",
      "\n",
      "\n",
      "Topics: 13, Coherence: 0.5608, Runtime: 3.39 minutes\n",
      "\n",
      "\n",
      "Topics: 14, Coherence: 0.5491, Runtime: 3.19 minutes\n",
      "\n",
      "\n",
      "Topics: 15, Coherence: 0.5608, Runtime: 3.30 minutes\n",
      "\n",
      "\n",
      "Topics: 16, Coherence: 0.5716, Runtime: 3.29 minutes\n",
      "\n",
      "\n",
      "Topics: 17, Coherence: 0.5345, Runtime: 3.27 minutes\n",
      "\n",
      "\n",
      "Topics: 18, Coherence: 0.5287, Runtime: 3.36 minutes\n",
      "\n",
      "\n",
      "Topics: 19, Coherence: 0.5541, Runtime: 3.23 minutes\n",
      "\n",
      "\n",
      "Topics: 20, Coherence: 0.5130, Runtime: 3.29 minutes\n",
      "\n",
      "\n",
      "Topics: 21, Coherence: 0.5550, Runtime: 3.27 minutes\n",
      "\n",
      "\n",
      "Topics: 22, Coherence: 0.5125, Runtime: 3.28 minutes\n",
      "\n",
      "\n",
      "Topics: 23, Coherence: 0.5115, Runtime: 3.31 minutes\n",
      "\n",
      "\n",
      "Topics: 24, Coherence: 0.5296, Runtime: 3.30 minutes\n",
      "\n",
      "\n",
      "Topics: 25, Coherence: 0.5327, Runtime: 3.32 minutes\n",
      "\n",
      "\n",
      "\n",
      "Full coherence test completed in 69.20 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Test different topic counts in terms of coherence\n",
    "\n",
    "results = coherence_test(dictionary, corpus, tokenized_bigrams, start= 5 , limit=25, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd959d-e932-4558-a67c-97c143ceef13",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Topics: 5, Coherence: 0.5289, Runtime: 3.27 minutes\n",
    "\n",
    "\n",
    "Topics: 6, Coherence: 0.5501, Runtime: 3.39 minutes\n",
    "\n",
    "\n",
    "Topics: 7, Coherence: 0.5097, Runtime: 3.32 minutes\n",
    "\n",
    "\n",
    "Topics: 8, Coherence: 0.5351, Runtime: 3.36 minutes\n",
    "\n",
    "\n",
    "Topics: 9, Coherence: 0.5346, Runtime: 3.39 minutes\n",
    "\n",
    "\n",
    "Topics: 10, Coherence: 0.6016, Runtime: 3.30 minutes\n",
    "\n",
    "\n",
    "Topics: 11, Coherence: 0.5253, Runtime: 3.19 minutes\n",
    "\n",
    "\n",
    "Topics: 12, Coherence: 0.5793, Runtime: 3.20 minutes\n",
    "\n",
    "\n",
    "Topics: 13, Coherence: 0.5608, Runtime: 3.39 minutes\n",
    "\n",
    "\n",
    "Topics: 14, Coherence: 0.5491, Runtime: 3.19 minutes\n",
    "\n",
    "\n",
    "Topics: 15, Coherence: 0.5608, Runtime: 3.30 minutes\n",
    "\n",
    "\n",
    "Topics: 16, Coherence: 0.5716, Runtime: 3.29 minutes\n",
    "\n",
    "\n",
    "Topics: 17, Coherence: 0.5345, Runtime: 3.27 minutes\n",
    "\n",
    "\n",
    "Topics: 18, Coherence: 0.5287, Runtime: 3.36 minutes\n",
    "\n",
    "\n",
    "Topics: 19, Coherence: 0.5541, Runtime: 3.23 minutes\n",
    "\n",
    "\n",
    "Topics: 20, Coherence: 0.5130, Runtime: 3.29 minutes\n",
    "\n",
    "\n",
    "Topics: 21, Coherence: 0.5550, Runtime: 3.27 minutes\n",
    "\n",
    "\n",
    "Topics: 22, Coherence: 0.5125, Runtime: 3.28 minutes\n",
    "\n",
    "\n",
    "Topics: 23, Coherence: 0.5115, Runtime: 3.31 minutes\n",
    "\n",
    "\n",
    "Topics: 24, Coherence: 0.5296, Runtime: 3.30 minutes\n",
    "\n",
    "\n",
    "Topics: 25, Coherence: 0.5327, Runtime: 3.32 minutes\n",
    "\n",
    "\n",
    "\n",
    "Full coherence test completed in 69.20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fa268356-fadd-462d-aa4b-32aa3bdd5d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 10, Coherence: 0.6016, Runtime: 3.74 minutes\n",
      "\n",
      "\n",
      "Topics: 15, Coherence: 0.5608, Runtime: 3.75 minutes\n",
      "\n",
      "\n",
      "Topics: 20, Coherence: 0.5123, Runtime: 3.71 minutes\n",
      "\n",
      "\n",
      "\n",
      "Full coherence test completed in 11.20 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Test different topic counts in terms of coherence\n",
    "\n",
    "results = coherence_test(dictionary, corpus, tokenized_bigrams, start= 10 , limit=20, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4857a-3179-4eaf-8c4f-adb3d160053e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 7, Coherence: 0.5097, Runtime: 3.23 minutes\n",
      "\n",
      "\n",
      "Topics: 9, Coherence: 0.5346, Runtime: 3.23 minutes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test other different topic counts in terms of coherence\n",
    "\n",
    "results_01 = coherence_test(dictionary, corpus, tokenized_bigrams, start= 7 , limit=13, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9347eee0-da52-450b-b904-701b00724388",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model fit in 3.414894700050354 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Fit LDA Model\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "lda = LdaModel(\n",
    "    corpus = corpus,\n",
    "    id2word = dictionary,\n",
    "    num_topics = 10,   # Tune\n",
    "    passes = 10,\n",
    "    random_state = 2025\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start)/60\n",
    "\n",
    "print(f\"LDA model fit in {runtime:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01d51d8d-fe63-4f75-8864-ffa0a1da7595",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.012*\"work\" + 0.011*\"wear\" + 0.010*\"not\" + 0.010*\"get\" + 0.010*\"wash\" + 0.009*\"would\" + 0.009*\"use\" + 0.009*\"like\" + 0.008*\"day\" + 0.008*\"need\"\n",
      "\n",
      "\n",
      "Topic 1: 0.045*\"size\" + 0.032*\"legging\" + 0.023*\"fit\" + 0.012*\"like\" + 0.011*\"would\" + 0.011*\"wunder_train\" + 0.011*\"align\" + 0.010*\"wear\" + 0.010*\"get\" + 0.010*\"waist\"\n",
      "\n",
      "\n",
      "Topic 2: 0.041*\"color\" + 0.026*\"anyone\" + 0.018*\"look\" + 0.016*\"like\" + 0.015*\"find\" + 0.014*\"see\" + 0.014*\"pic\" + 0.013*\"photo\" + 0.013*\"item\" + 0.012*\"picture\"\n",
      "\n",
      "\n",
      "Topic 3: 0.046*\"black\" + 0.036*\"align\" + 0.020*\"white\" + 0.015*\"short\" + 0.012*\"high_rise\" + 0.012*\"skirt\" + 0.012*\"color\" + 0.012*\"bra\" + 0.011*\"wunder_train\" + 0.011*\"espresso\"\n",
      "\n",
      "\n",
      "Topic 4: 0.134*\"size\" + 0.038*\"bra\" + 0.031*\"fit\" + 0.026*\"top\" + 0.018*\"tank\" + 0.014*\"shirt\" + 0.014*\"align\" + 0.012*\"sizing\" + 0.011*\"align_tank\" + 0.010*\"wear\"\n",
      "\n",
      "\n",
      "Topic 5: 0.141*\"short\" + 0.074*\"pant\" + 0.038*\"men\" + 0.026*\"jogger\" + 0.023*\"softstreme\" + 0.020*\"woman\" + 0.018*\"logo\" + 0.017*\"shirt\" + 0.015*\"length\" + 0.015*\"anyone_know\"\n",
      "\n",
      "\n",
      "Topic 6: 0.045*\"jacket\" + 0.037*\"scuba\" + 0.021*\"define_jacket\" + 0.019*\"bone\" + 0.019*\"one\" + 0.013*\"cropped\" + 0.013*\"half_zip\" + 0.012*\"define\" + 0.012*\"like\" + 0.011*\"anyone\"\n",
      "\n",
      "\n",
      "Topic 7: 0.041*\"new\" + 0.041*\"bag\" + 0.016*\"item\" + 0.014*\"thread\" + 0.014*\"color\" + 0.013*\"drop\" + 0.010*\"backpack\" + 0.010*\"belt_bag\" + 0.009*\"nomad\" + 0.009*\"see\"\n",
      "\n",
      "\n",
      "Topic 8: 0.020*\"legging\" + 0.019*\"pair\" + 0.017*\"like\" + 0.016*\"one\" + 0.016*\"aligns\" + 0.015*\"love\" + 0.015*\"color\" + 0.015*\"new\" + 0.013*\"not\" + 0.012*\"got\"\n",
      "\n",
      "\n",
      "Topic 9: 0.027*\"store\" + 0.021*\"item\" + 0.014*\"not\" + 0.013*\"sale\" + 0.012*\"get\" + 0.011*\"return\" + 0.011*\"order\" + 0.010*\"online\" + 0.008*\"got\" + 0.008*\"one\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show topics\n",
    "\n",
    "for idx, topic in lda.print_topics(num_topics=10, num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63c400-9ffd-4ff1-9fac-c72130a4f429",
   "metadata": {},
   "source": [
    "## Modeling with BERTopic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lulu_lda_env)",
   "language": "python",
   "name": "lulu_lda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
