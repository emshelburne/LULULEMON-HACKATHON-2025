{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd2fb6e-91ee-4c89-96a2-64553fb7fa4a",
   "metadata": {},
   "source": [
    "# Topic Modeling with BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab3483-7b13-48f0-93d5-e6afb0ee2d46",
   "metadata": {},
   "source": [
    "In this notebook, we undergo topic modeling with the state of the art language model BERTopic. After hyperparameter tuning, we select an optimal choice of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45f844-7678-47d9-81c9-492d8571f422",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac48e06-1b3a-4dd8-ba25-f8cc9daee548",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# ============ General ============\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Optional, List\n",
    "import math\n",
    "from itertools import chain\n",
    "\n",
    "# ============ Plotting ============\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "# ============ Text Preprocessing  ============\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# spaCy for lemmatization/POS filtering\n",
    "try:\n",
    "    import spacy\n",
    "    _SPACY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    _SPACY_AVAILABLE = False\n",
    "\n",
    "# ============ BERTopic stack ============\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Repro + warnings\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "# for _res in [\"stopwords\", \"wordnet\", \"omw-1.4\", \"punkt\"]:\n",
    "#     try:\n",
    "#         nltk.data.find(f\"corpora/{_res}\")\n",
    "#     except LookupError:\n",
    "#         nltk.download(_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa28980-b7fb-4c70-92dd-ff0903377914",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set user's data path\n",
    "\n",
    "PATH = f\"C:/Users/emshe/Desktop/BRAINSTATION/LULULEMON/DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3de349-221d-42dd-92ac-bf8a5066d110",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Download NLTK files (run once)\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7c212-fa67-40e1-872f-bfd63fe1382a",
   "metadata": {},
   "source": [
    "## Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7ec57d-8299-43dd-9343-252d47d0bae0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "\n",
    "def clean_text(s: str | None) -> str | None:\n",
    "    \n",
    "    '''\n",
    "    Clean string by substituting spaces for problematic characters\n",
    "    '''\n",
    "    \n",
    "    if s is None:\n",
    "        return None\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b388f449-e518-4bf3-8680-e6024a8e3608",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to get datetime from UTC timestamp\n",
    "\n",
    "def dt_from_epoch(ts: Optional[int]):\n",
    "\n",
    "    \"\"\"\n",
    "    Convert timestamp to pd.datetime format\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if ts is None:\n",
    "        return None\n",
    "    return pd.to_datetime(ts, unit=\"s\", utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31afab8-4692-43da-87bc-0723484c41b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to examine dataframes\n",
    "\n",
    "def examine_df(name,df,\n",
    "               include_stats = True,\n",
    "               include_sample = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check basic info about a dataframe df\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n\\nNumber of records in the {name} is: {len(df)}\\n\")\n",
    "    print(f\"\\nNumber of features in the {name} is: {len(df.columns)}\\n\")\n",
    "    print(f\"The columns in the {name} are: {df.columns}\\n\")\n",
    "    print(f\"\\n Other info about {name}:\\n\")\n",
    "    display(df.info())\n",
    "    if include_stats == True:\n",
    "        print(f'\\n Basic statistical info about {name}:\\n')\n",
    "        display(df.describe())\n",
    "    if include_sample == True:\n",
    "        print(f\"\\n\\nSample of records in the {name}:\")\n",
    "        display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e964fb-cd5c-4f31-a3de-b34833a66d2d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to get sample from text column\n",
    "\n",
    "def get_text_samples(df: pd.DataFrame, text_col: str, n: int) -> None:\n",
    "\n",
    "    '''\n",
    "    Print n samples from a text column in a dataframe\n",
    "    '''\n",
    "\n",
    "    # Ensure pandas doesn't truncate text\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    # Sample and print 5 full negative reviews\n",
    "    print(\"Sample text data:\\n\\n\")\n",
    "    sample = df[text_col].sample(n)\n",
    "    for i, description in enumerate(sample, 1):\n",
    "        print(f\"Text sample {i}:\\n\\n\\n{description}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37be0d6-5f22-4ed3-87e1-b4a69677e0b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function for categorical bar graph\n",
    "\n",
    "def bar_graph(df: pd.DataFrame, col: str) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Generate bar graph for categorical column in a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in dataframe\")\n",
    "\n",
    "    counts = posts_df[col].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(f\"Distribution of {col.title()}\")\n",
    "    plt.xlabel(f\"{col.title()}\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8ae594-562e-49e5-a6e3-0f7c126993ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define function to plot histogram for numeric columns\n",
    "\n",
    "def histogram(df: pd.DataFrame, \n",
    "             col: str,\n",
    "            bins: int = 30,\n",
    "             log: bool = False) -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a histogram for a numeric column in a dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in dataframe\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    df[col].dropna().hist(bins=bins, edgecolor=\"black\", log=log)\n",
    "    plt.title(f\"Histogram of {col.title()}\")\n",
    "    plt.xlabel(col.title())\n",
    "    plt.ylabel(\"Log(Frequency)\" if log else \"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a314a84b-a9ef-4b8c-b3f7-6fc4a06bb7b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to load ndjson\n",
    "\n",
    "def load_plain_ndjson(path: str, limit: Optional[int] = None) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Load a plain-text NDJSON file line by line into a DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            rows.append({\n",
    "                \"post_id\": obj.get(\"id\"),\n",
    "                \"timestamp\": dt_from_epoch(obj.get(\"created_utc\")),\n",
    "                \"author\": obj.get(\"author\"),\n",
    "                \"title\": obj.get(\"title\"),\n",
    "                \"text\": obj.get(\"selftext\"),\n",
    "                \"score\": obj.get(\"score\"),\n",
    "                \"num_comments\": obj.get(\"num_comments\"),\n",
    "                \"permalink\": obj.get(\"permalink\"),\n",
    "                \"subreddit\": obj.get(\"subreddit\"),\n",
    "            })\n",
    "\n",
    "            if limit and i >= limit:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc6245-b542-4dd5-8e0d-0f05a02f765f",
   "metadata": {},
   "source": [
    "## Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efffc6ad-ecdd-4a46-8730-a7ea402b83a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load clean data\n",
    "\n",
    "lulu_df = pd.read_parquet(f\"{PATH}/lululemon_submissions_clean.parquet\", engine = 'fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0925c71e-8b94-41d1-a28b-f6df482c8149",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of records in the lulu dataframe is: 57984\n",
      "\n",
      "\n",
      "Number of features in the lulu dataframe is: 6\n",
      "\n",
      "The columns in the lulu dataframe are: Index(['post_id', 'timestamp', 'title', 'text', 'score', 'num_comments'], dtype='object')\n",
      "\n",
      "\n",
      " Other info about lulu dataframe:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57984 entries, 0 to 57983\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype              \n",
      "---  ------        --------------  -----              \n",
      " 0   post_id       57984 non-null  object             \n",
      " 1   timestamp     57984 non-null  datetime64[ns, UTC]\n",
      " 2   title         57984 non-null  object             \n",
      " 3   text          57984 non-null  object             \n",
      " 4   score         57984 non-null  int64              \n",
      " 5   num_comments  57984 non-null  int64              \n",
      "dtypes: datetime64[ns, UTC](1), int64(2), object(3)\n",
      "memory usage: 2.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Basic statistical info about lulu dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>57984.000000</td>\n",
       "      <td>57984.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.446071</td>\n",
       "      <td>14.705126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>87.240166</td>\n",
       "      <td>40.279924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11864.000000</td>\n",
       "      <td>1987.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score  num_comments\n",
       "count  57984.000000  57984.000000\n",
       "mean      23.446071     14.705126\n",
       "std       87.240166     40.279924\n",
       "min        0.000000      0.000000\n",
       "25%        1.000000      2.000000\n",
       "50%        3.000000      6.000000\n",
       "75%       13.000000     13.000000\n",
       "max    11864.000000   1987.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sample of records in the lulu dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eielly</td>\n",
       "      <td>2020-01-01 05:33:25+00:00</td>\n",
       "      <td>Monthly Sales Post- January</td>\n",
       "      <td>FS: Aligns sz 4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eii06s</td>\n",
       "      <td>2020-01-01 12:46:35+00:00</td>\n",
       "      <td>Major problem falling down leggings?</td>\n",
       "      <td>Hello, over the last year I have been ordering...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eijtca</td>\n",
       "      <td>2020-01-01 16:00:56+00:00</td>\n",
       "      <td>Tops for yoga</td>\n",
       "      <td>I have a couple swiftly tech racerbacks for ho...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eikiew</td>\n",
       "      <td>2020-01-01 16:59:27+00:00</td>\n",
       "      <td>ABC Pants - Sizing</td>\n",
       "      <td>Hey all,\\n\\nI recently received ABC pants (siz...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eil4bb</td>\n",
       "      <td>2020-01-01 17:46:20+00:00</td>\n",
       "      <td>Certain Aligns colours with thicker fabric?</td>\n",
       "      <td>Hi lemonheads :D\\n\\nI was wondering if anyone ...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                 timestamp  \\\n",
       "0  eielly 2020-01-01 05:33:25+00:00   \n",
       "1  eii06s 2020-01-01 12:46:35+00:00   \n",
       "2  eijtca 2020-01-01 16:00:56+00:00   \n",
       "3  eikiew 2020-01-01 16:59:27+00:00   \n",
       "4  eil4bb 2020-01-01 17:46:20+00:00   \n",
       "\n",
       "                                         title  \\\n",
       "0                  Monthly Sales Post- January   \n",
       "1         Major problem falling down leggings?   \n",
       "2                                Tops for yoga   \n",
       "3                           ABC Pants - Sizing   \n",
       "4  Certain Aligns colours with thicker fabric?   \n",
       "\n",
       "                                                text  score  num_comments  \n",
       "0                                    FS: Aligns sz 4      1             7  \n",
       "1  Hello, over the last year I have been ordering...      0             6  \n",
       "2  I have a couple swiftly tech racerbacks for ho...      3             4  \n",
       "3  Hey all,\\n\\nI recently received ABC pants (siz...      1             6  \n",
       "4  Hi lemonheads :D\\n\\nI was wondering if anyone ...      3            11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine data\n",
    "\n",
    "examine_df('lulu dataframe', lulu_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5177d54-a7f7-4cdd-9b92-68cfc30587a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Copy original dataframe\n",
    "\n",
    "og_lulu_df = lulu_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6023f0-b33e-459d-9c91-7bb61ba1f393",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad46964f-0897-4e64-9146-698a27e71aaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reset dataframe\n",
    "\n",
    "lulu_df = og_lulu_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73e6c995-fd90-4ae2-92fb-7abf829b6599",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define stop word list, lemmatizer, and regex\n",
    "\n",
    "# Stopwords\n",
    "\n",
    "custom_stop_words = [\n",
    "    # brand/boilerplate\n",
    "    \"lululemon\", \"lulu\", \"amp\", \"xx\", \"lol\",\n",
    "    \"like\", \"get\", \"got\", \"would\", \"anyone\", \"one\",\n",
    "\n",
    "    # deletion/removal artifacts\n",
    "    \"deleted\", \"remove\", \"removed\", \"removal\",\n",
    "    \"deleted_view\", \"removed_view\", \"view_poll\", \"poll_view\",\n",
    "    \"deleted_view_poll\", \"removed_view_poll\",\n",
    "    \"view\", \"poll\", \"results\", \"result\", \"vote\", \"votes\",\n",
    "    \"thread\", \"post\", \"posting\", \"posted\", \"comment\", \"comments\",\n",
    "\n",
    "    # generic low-information Reddit junk\n",
    "    \"http\", \"https\", \"www\", \"com\",\n",
    "    \"imgur\", \"jpg\", \"png\", \"gif\",\n",
    "    \"subreddit\", \"reddit\", \"mod\", \"mods\",\n",
    "    \"link\", \"links\",\n",
    "\n",
    "    # Scraped filler \n",
    "    \"user\", \"account\", \"profile\",\n",
    "    \"page\", \"site\", \"website\",\n",
    "    \"viewed\", \"views\", \"seen\"\n",
    "]\n",
    "\n",
    "base_stops = set(stopwords.words(\"english\"))\n",
    "base_stops -= {\"no\", \"nor\", \"not\", \"never\"}       # Keep negations\n",
    "\n",
    "stop_words = list(base_stops.union(custom_stop_words))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Precompile regex\n",
    "_link = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "_nonalpha = re.compile(r'[^a-z\\s]')\n",
    "_spaces = re.compile(r'\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8d60460-b4d5-4a6b-8630-198dca17822d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define text preprocessor\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess text before modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = _link.sub(\" \", text)         # Remove links\n",
    "    text = _nonalpha.sub(\" \", text)     # Keep only letters/spaces\n",
    "    tokens = []\n",
    "    for t in text.split():\n",
    "        if t in stop_words or len(t) < 3:\n",
    "            continue\n",
    "        t = lemmatizer.lemmatize(t)\n",
    "        tokens.append(t)\n",
    "    return _spaces.sub(\" \", \" \".join(tokens)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72a9bea5-1541-4a16-8e77-7f8beb48bb9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Apply text preprocessing\n",
    "\n",
    "lulu_df[\"clean_text\"] = lulu_df[\"title\"].fillna(\"\") + \" \" + lulu_df[\"text\"].fillna(\"\")\n",
    "lulu_df[\"clean_text\"] = lulu_df[\"clean_text\"].apply(preprocess)\n",
    "\n",
    "# drop docs with <5 tokens to reduce noise\n",
    "lulu_df = lulu_df[lulu_df[\"clean_text\"].str.split().str.len() >= 5].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46d8da26-6a21-4296-9af5-c2328ec2da07",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text data:\n",
      "\n",
      "\n",
      "Text sample 1:\n",
      "\n",
      "\n",
      "aligned midi dress recommendation size work appropriate anybody aligned midi dress wmtm canada brunch back dress wore work hooked love dress another cut colour wear work slit back crazy high anybody love tried purchase\n",
      "\n",
      "\n",
      "\n",
      "Text sample 2:\n",
      "\n",
      "\n",
      "align short restock alert align short black back stock size look restocked size\n",
      "\n",
      "\n",
      "\n",
      "Text sample 3:\n",
      "\n",
      "\n",
      "cat owner furry friend snag life little rant try not hold cat wearing anything snag easily swiftlys aligns define jacket name piece wear cuddle cat scuba since thicker guess putting zip scuba today noticed many snag snag actually ripped meaning loop happens get snagged get ripped half make sense love cat death anywhere near wearing lmao picture little destroyer\n",
      "\n",
      "\n",
      "\n",
      "Text sample 4:\n",
      "\n",
      "\n",
      "dark lavender best colour flattering skin tone vibrant still classic received softstreme set poshmark add collection excited never coloured legging not look double lined dark enough not show much texture love sweetheart bra aligns align short pocket size\n",
      "\n",
      "\n",
      "\n",
      "Text sample 5:\n",
      "\n",
      "\n",
      "lavender dew unders thought whether lavender dew might appear wu not fan double lined aligns\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check some examples\n",
    "\n",
    "get_text_samples(lulu_df, 'clean_text', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad115157-1c09-4811-a52a-727a98f748aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Collect tokenized docs\n",
    "\n",
    "tokenized_docs = [doc.split() for doc in lulu_df[\"clean_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16994c30-0aa0-4fc8-9c39-54fdb700971b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to merge bigrams\n",
    "\n",
    "def merge_bigrams(doc, bigram_set):\n",
    "    \n",
    "    \"\"\"\n",
    "    Merge bigrams\n",
    "    \"\"\"\n",
    "    \n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(doc):\n",
    "        if i < len(doc)-1 and (doc[i], doc[i+1]) in bigram_set:\n",
    "            merged.append(f\"{doc[i]}_{doc[i+1]}\")\n",
    "            i += 2\n",
    "        else:\n",
    "            merged.append(doc[i])\n",
    "            i += 1\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ae769bd-05cb-49af-be18-116d1fe5b5c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Apply bigram detector\n",
    "\n",
    "bigrams = [list(ngrams(doc, 2)) for doc in tokenized_docs]\n",
    "flat_bigrams = [bg for doc in bigrams for bg in doc]\n",
    "bigram_counts = Counter(flat_bigrams)\n",
    "common_bigrams = {bg for bg, count in bigram_counts.items() if count >= 10}\n",
    "\n",
    "tokenized_bigrams = [merge_bigrams(doc, common_bigrams) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de8ee4f7-834a-4222-ac7e-cfd09c469f9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Add bigram text back to dataframe\n",
    "\n",
    "lulu_df[\"clean_text_bigram\"] = [\" \".join(doc) for doc in tokenized_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "284f27ce-6c58-4933-9182-41c4dec0cede",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text data:\n",
      "\n",
      "\n",
      "Text sample 1:\n",
      "\n",
      "\n",
      "know totally_different could_keep considering versatility item_work weather justification welcome\n",
      "\n",
      "\n",
      "\n",
      "Text sample 2:\n",
      "\n",
      "\n",
      "loving black_fleece ebb gold really pop black great every_day bag\n",
      "\n",
      "\n",
      "\n",
      "Text sample 3:\n",
      "\n",
      "\n",
      "question dude male athletic_build love clothing seems woman_pant tights greater quality men seem thicker nicer city_sweat discipline surge pant_fit great discipline seem borderline resemble quality yoga_pant kinda wishing take woman_line add fabric_not tight_also feel_way\n",
      "\n",
      "\n",
      "\n",
      "Text sample 4:\n",
      "\n",
      "\n",
      "wonder chrome_aligns price_drop since one buying sorry another_align\n",
      "\n",
      "\n",
      "\n",
      "Text sample 5:\n",
      "\n",
      "\n",
      "ootd twilight_rose java half sleeve close_body shelf shirt java_size cinchable_waist high_rise woven_short size twilight_rose shirt soooo_soft love shelf_bra beautiful open_back short comfy\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check some examples\n",
    "\n",
    "get_text_samples(lulu_df, 'clean_text_bigram', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65684850-18d9-4d51-95a3-4506a0cc21b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get list of docs\n",
    "\n",
    "docs = lulu_df[\"clean_text_bigram\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63c400-9ffd-4ff1-9fac-c72130a4f429",
   "metadata": {},
   "source": [
    "## Modeling with BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63c58a7f-7a31-41d2-8d41-33bfae998da0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to examine basic topic info\n",
    "\n",
    "def examine_topics(topic_model, topics, probs):\n",
    "    \n",
    "    # Topic table and basic stats\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topic_info.head(10)\n",
    "    \n",
    "    # Number of discovered topics (exclude -1 = outliers)\n",
    "    n_topics = int((topic_info[\"Topic\"] != -1).sum())\n",
    "    n_docs = len(docs)\n",
    "    outlier_share = (topics.count(-1) / n_docs) if n_docs else 0.0\n",
    "    \n",
    "    print(f\"Discovered topics (excl. -1): {n_topics}\")\n",
    "    print(f\"Outlier docs (-1): {topics.count(-1)} / {n_docs} = {outlier_share:.1%}\")\n",
    "    \n",
    "    # Topic size distribution\n",
    "    display(topic_info[[\"Topic\", \"Count\"]].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bdaf1eac-83d6-4ae7-b041-11b4ac0ba378",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to check top words for each topic\n",
    "\n",
    "\n",
    "def check_top_words(topic_model, topics, probs):\n",
    "    # Refresh topic_info from the current model\n",
    "    topic_info = topic_model.get_topic_info().copy()\n",
    "    \n",
    "    # Get all non-noise topics\n",
    "    topic_ids = topic_info.loc[topic_info[\"Topic\"] != -1, \"Topic\"].tolist()\n",
    "    \n",
    "    for t in topic_ids:\n",
    "        # safe size lookup\n",
    "        size = int(topic_info.loc[topic_info[\"Topic\"] == t, \"Count\"].iloc[0])\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nTopic {t} | size={size}\")\n",
    "    \n",
    "        # top words (guard if topic exists but empty)\n",
    "        words = topic_model.get_topic(t) or []\n",
    "        top_terms = \", \".join([w for w, _ in words[:15]]) if words else \"(no terms)\"\n",
    "        print(\"\\nTop words:\", top_terms)\n",
    "    \n",
    "        # representative examples (guard empty)\n",
    "        reps = (topic_model.get_representative_docs(t) or [])[:3]\n",
    "        for i, doc in enumerate(reps, 1):\n",
    "            preview = doc[:300].replace(\"\\n\", \" \")\n",
    "            suffix = \"...\" if len(doc) > 300 else \"\"\n",
    "            print(f\"\\n--- Ex{i}: {preview}{suffix}\")\n",
    "    \n",
    "    print(\"\\nNon-noise topics:\", topic_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b02223ad-717e-4bee-8fac-ffe2741ef9e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load baseline sentence embedding model\n",
    "\n",
    "EMB_NAME = \"all-MiniLM-L6-v2\"\n",
    "st_model = SentenceTransformer(EMB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e964b2f-3099-4d6a-821c-86bfc3974fd3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346de6071c414514a867007e774e059b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1768 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "\n",
    "embs = st_model.encode(\n",
    "    docs,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True  # Normalizes vectors for cosine sim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e567c163-f008-4c86-b17e-e52fa0102513",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Configure UMAP model\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors = 30,\n",
    "    n_components=5,\n",
    "    min_dist = 0.20,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d71dba56-5c5f-45d8-a1b8-8997edfdcf35",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Configure DBSCAN model\n",
    "\n",
    "N = len(docs)\n",
    "\n",
    "min_cluster_size = max(200, math.floor(0.015 * N))  # ≈0.5% of corpus, at least 50\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples= 35,  # defaults to min_cluster_size; set e.g. 10–30 to merge a bit more\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    "    cluster_selection_epsilon = 0.00\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab543562-e52b-4e20-9b93-29b957a9f6ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define vectorizer to keep unigrams and bigrams\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"(?u)\\b[\\w_]{3,}\\b\",\n",
    "    stop_words = None,\n",
    "        max_df = .9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "139b7a14-b8a6-4699-a2e3-7ea83a621576",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Build BERTopic model\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=st_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language=\"english\",\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    top_n_words=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3f7cfd8-9a43-4329-a164-69cbff74613a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered topics (excl. -1): 11\n",
      "Outlier docs (-1): 36449 / 56574 = 64.4%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>36449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>1825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>1604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count\n",
       "0      -1  36449\n",
       "1       0   3354\n",
       "2       1   2206\n",
       "3       2   2140\n",
       "4       3   1998\n",
       "5       4   1852\n",
       "6       5   1825\n",
       "7       6   1686\n",
       "8       7   1604\n",
       "9       8   1452\n",
       "10      9   1034\n",
       "11     10    974"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full runtime: 2.14 minutes.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit BERTopic model\n",
    "\n",
    "start = time.time()\n",
    "    \n",
    "topic_model, topics, probs = run_bertopic(docs, embs, st_model)\n",
    "    \n",
    "end = time.time()\n",
    "    \n",
    "runtime = (end-start)/60\n",
    "print(f\"Full runtime: {runtime:.2f} minutes.\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29ac2a5c-c340-4bea-96f3-0a03c1dccacb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Topic 0 | size=3354\n",
      "\n",
      "Top words: bra, sport_bra, energy_bra, cup, ebb_street, high_neck, boob, bra_size, strap, tank_top, cloud_bra, long_line, chest, free_serene, pad\n",
      "\n",
      "--- Ex1: cup lady high_neck energy_bra tit absolutely none interested high_neck energy_bra cup lady could talk fit_usually size_bra top_wondering run_big cup thanks flat_chested friend advance\n",
      "\n",
      "--- Ex2: prefer bra removable_cup built cup honestly prefer built cup worry adjusting usually see_outline pad either since built think_bra also combat uni boob providing shape structure wore bra built cup year_however brand mainly offer bra removable_pad give\n",
      "\n",
      "--- Ex3: difference nonsale energy_bra longline cup wmtm cup energy_bra longline bought floral patterned energy_longline bra_wmtm today didnt pay_much attention noticing say_cup swear_saw bra yesterday full_price listed cup think error worried cup energy_bra longline know_fit definitely cup_version\n",
      "================================================================================\n",
      "\n",
      "Topic 1 | size=2206\n",
      "\n",
      "Top words: gift_card, card, fedex, refund, shipped, shipping, gec, delivered, receipt, delivery, customer_service, sent, cancelled, tracking, refunded\n",
      "\n",
      "--- Ex1: gift_card stolen customer_service awesome bought wife gift_card bday live mile nearest_store knew wanted browse find ton_stuff checkout gift_card cashier state open gift_card package scan back opened slid denied tried denied shock receipt bought see transaction bank transaction cashier hand phone_nu...\n",
      "\n",
      "--- Ex2: worst customer educator return_experience ever april ordered_jacket gift sister credit_card promptly charged received jacket_not fit due_covid store_closed returned_item may option gift_card credit email jacket received may credit received jun called hold min informed return_process business_day iss...\n",
      "\n",
      "--- Ex3: fyi return headache wanted_share recent_experience holiday bought quite thing_online loving girl life plus gift mostly full_price wmtm_went town family week settling back_time make final decision anything needed returned holiday return period end ended thing spread across_different order needed back...\n",
      "================================================================================\n",
      "\n",
      "Topic 2 | size=2140\n",
      "\n",
      "Top words: liner, speed_ups, sizing_help, hip, waistband, thigh, waist_hip, butt, compression, pace_breaker, tracker_short, thong, first_pair, measurement, biker_short\n",
      "\n",
      "--- Ex1: speed_ups men_wear speed_ups\n",
      "\n",
      "--- Ex2: men_running short sell brief style liner anymore sizing full liner need_new pair_short never run full_length liner tight_fit really brief style liner guess_not thing anymore\n",
      "\n",
      "--- Ex3: help liner pink_speed ups love hardly wear liner really unflattering behind granny panty_line next_size big help liner situation saw_bunch tiktok people boat cutting_liner feel liner wanted keep_trying fix panty_line situation last resort guess cut_liner wear_want cut much though saw_someone cut par...\n",
      "================================================================================\n",
      "\n",
      "Topic 3 | size=1998\n",
      "\n",
      "Top words: wunder_puff, coat, always_effortless, rain, hooded_define, parka, cold_weather, define, luon_define, jacket_size, cross_chill, cropped_define, waterproof, jacket_sizing, rain_rebel\n",
      "\n",
      "--- Ex1: jacket pnw ordered insulated rain_rebel coat think warm_enough seems thin parka seem warm dont_think want puffer coat long puffer outside long_period looking_something wear car errand work walk withstand rain light snow suggestion\n",
      "\n",
      "--- Ex2: wunder_puff waist anybody shed_light wunder_puff waist coat fit_pic seat belt belt think_want know pack long coat practical hardly review_help\n",
      "\n",
      "--- Ex3: snow_warrior parka winter_warrior parka know_difference two coat better cold saw snow_warrior parka store_not winter fit\n",
      "================================================================================\n",
      "\n",
      "Topic 4 | size=1852\n",
      "\n",
      "Top words: strawberry_milkshake, color_comparison, comparison, legacy_green, shade, colour_comparison, iron_blue, color_comparison request, dark_forest, darker, flush_pink, diamond_dye, lavender_dew, scuba, yellow\n",
      "\n",
      "--- Ex1: know strawberry_milkshake strawberry_milkshake align_tank moonlit_magenta align_legging\n",
      "\n",
      "--- Ex2: color_comparison request color_comparison request charged_indigo psychic\n",
      "\n",
      "--- Ex3: strawberry_milkshake mistake strawberry_milkshake align_short\n",
      "================================================================================\n",
      "\n",
      "Topic 5 | size=1825\n",
      "\n",
      "Top words: ready_rulu, scuba_jogger, adapted_state, abc_jogger, inseam, abc_pant, surge_jogger, slim_fit, slim, tall, ankle, commission_pant, dance_studio jogger, men_jogger, rise_jogger\n",
      "\n",
      "--- Ex1: stretch_high rise_jogger inseam looking stretch_high rise_jogger full_length state inseam_size checked_size chart said inseam think accurate\n",
      "\n",
      "--- Ex2: city_sweat jogger_sizing friend saw pink_lychee city_sweat jogger markdown wondering_size wear_legging scuba_jogger ready_rulu jogger_dance studio_jogger room\n",
      "\n",
      "--- Ex3: adapted_state jogger_know inseam adapted_state jogger\n",
      "================================================================================\n",
      "\n",
      "Topic 6 | size=1686\n",
      "\n",
      "Top words: belt_bag, backpack, everywhere_belt, ebb, strap, city_adventurer, everywhere_belt bag, water_bottle, pouch, tote, crossbody, mini, crossbody_bag, new_crew, wristlet\n",
      "\n",
      "--- Ex1: city_adventurer belt_bag strap extender know strap extender fit city_adventurer belt_bag smidge short strap extender regular belt_bag fit_thanks\n",
      "\n",
      "--- Ex2: city_adventurer backpack received city_adventurer backpack today_noticed unpackaged padding back laptop pocket_also sits back wearing backpack flat kind warped run hand along side_side large bump know_normal even rid bump felt_wearing backpack laptop stuff inside note tag_still backpack used yet_tha...\n",
      "\n",
      "--- Ex3: unreleased new refined city_adventurer bag hand released designed refined version city_adventurer backpack little_bit disappointed looking_getting city_adventurer backpack week_ago purchased return strap way_tight took backpack store_return noticed slightly_different version city_adventurer never re...\n",
      "================================================================================\n",
      "\n",
      "Topic 7 | size=1604\n",
      "\n",
      "Top words: scuba, hoodie, full_zip, zip, scuba_oversized, funnel_neck, scuba_hoodie, scuba_half, scuba_zip, hoodies, scuba_full, xxl, zip_hoodie, zip_scuba, oversized_scuba\n",
      "\n",
      "--- Ex1: scuba_half zip help_hello everyone_question scuba_zip hoodie opinion hoodie guy scuba_full zip awesome thanks_everyone\n",
      "\n",
      "--- Ex2: funnel_neck scuba_full zip army_green love army_green full_zip scuba ish usually scuba prefer_cropped full_length funnel_neck scuba tempted order trench australia\n",
      "\n",
      "--- Ex3: scuba_oversized full_zip hoodie storm_teal fave\n",
      "================================================================================\n",
      "\n",
      "Topic 8 | size=1452\n",
      "\n",
      "Top words: stain, smell, detergent, water, laundry, dryer, dry, bleach, stained, washer, washing_machine, soap, cleaning, scent, vinegar\n",
      "\n",
      "--- Ex1: love_scuba stain happening know stain anything gotten stain washing look oil_stain happen_else\n",
      "\n",
      "--- Ex2: not repeat not_use dawn_dish soap light_colored nulu_fabric posterity use dawn_dish soap sometimes stain_remover clothing year episode damage left blue dish_soap pink_shirt died blue left overnight leave long use blue soap right well bra kohlrabi color_pink dawn_dish soap supposed gentle hand chain ...\n",
      "\n",
      "--- Ex3: rid stain see stain laundry\n",
      "================================================================================\n",
      "\n",
      "Topic 9 | size=1034\n",
      "\n",
      "Top words: wunder, unders, contour_fit, compressive, wts, pair_wunder, aligns_wunder, size_wunder, waistband, compression, invigorates, contour, thigh, train_short, align_wunder\n",
      "\n",
      "--- Ex1: similar dimension help_sizing wunder_train legging_lb waist_hip currently_wearing size_wunder train consider thigh athletic side small calf past_year gained_weight originally purchased_legging around_lb still wunder_train feel_bit compressive noticed waistband starting roll little recently_ordered s...\n",
      "\n",
      "--- Ex2: size_recommendation wunder_train wunder_train legging supposed_fit think_wear many_align legging_maybe not_used compression typically_wear size_align legging sometimes buy_long double_lined tried_wunder train felt_much tighter lot_less compressive ended_going contour_fit still pull lot normal wunder...\n",
      "\n",
      "--- Ex3: align_wunder sizing_fast free recently_gifted pair_fast free_tights size_fit perfectly looking_getting loungey comfortable_legging next narrowed aligns_wunder unders however review post online_seem suggest_sizing aligns staying tt sizing_wunder unders_size aligns_big size_wunder unders small try_sto...\n",
      "================================================================================\n",
      "\n",
      "Topic 10 | size=974\n",
      "\n",
      "Top words: black_friday, outlet_haul, outlet_store, drive, send_sale, orlando_outlet, visiting, inventory, shipping, toronto, outlet_find, outlet_recently, visit, cyber_monday, opening\n",
      "\n",
      "--- Ex1: black_friday discussion_everyone icymi early_access drop_happened yesterday list_item dropped price linked reminder exclusively_app need apple device access ended wanted start new discus coming next next black_friday online_drop happen wednesday around early_access item_drop well additional item_edu...\n",
      "\n",
      "--- Ex2: black_friday official_daily question_everyone sudden influx black_friday question popping sub feed decided best concentrate central super informative place black_friday sale officially case_missed saw first part drop hit last_night addition new_uploads latest upload thread first_drop second drop_hap...\n",
      "\n",
      "--- Ex3: black_friday black_friday sale\n",
      "\n",
      "Non-noise topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Check top words\n",
    "\n",
    "check_top_words(topic_model, topics, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f71313-0a08-48e5-93cb-b4c6ea59b92a",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning and Grid Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2945dd40-7eb3-422e-b689-61000078925c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function for grid searches\n",
    "\n",
    "def run_once(mcs, ms, eps=0.02):\n",
    "    hdb = HDBSCAN(\n",
    "        min_cluster_size=mcs,\n",
    "        min_samples=ms,\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method=\"eom\",\n",
    "        prediction_data=True,\n",
    "        cluster_selection_epsilon=eps\n",
    "    )\n",
    "    vec = CountVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        token_pattern=r\"(?u)\\b[\\w_]{3,}\\b\",\n",
    "        stop_words=None,\n",
    "        min_df=1,\n",
    "        max_df=1.0\n",
    "    )\n",
    "    mdl = BERTopic(\n",
    "        umap_model=umap_sub,\n",
    "        hdbscan_model=hdb,\n",
    "        vectorizer_model=vec,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    t, _ = mdl.fit_transform(docs_big, embs_big)\n",
    "    info = mdl.get_topic_info()\n",
    "    k = int((info.Topic != -1).sum())\n",
    "    noise = float((np.array(t) == -1).mean())\n",
    "    print(f\"mcs={mcs:4d}, min_samples={ms:2d}, eps={eps:.2f}  →  subtopics={k:2d}, noise={noise:.1%}\")\n",
    "    return (k, noise, mdl, t, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b38bf163-2831-4bff-b0d5-be23ccff62e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Hyper parameter tuning function\n",
    "\n",
    "def run_bertopic(\n",
    "    docs, embs, st_model,\n",
    "    n_neighbors: int = 30,\n",
    "    min_dist: float = 0.20,\n",
    "    min_cluster_size: int | None = None,\n",
    "    min_samples: int = 35,\n",
    "    cluster_selection_epsilon: float = 0.00,\n",
    "    random_state: int = 42,\n",
    "    top_n_words: int = 15,\n",
    "    method: str = 'leaf'\n",
    "):\n",
    "    \"\"\"\n",
    "    Build & fit a BERTopic model with simple, optional overrides for UMAP/HDBSCAN.\n",
    "    Returns: topic_model, topics, probs\n",
    "    \"\"\"\n",
    "    N = len(docs)\n",
    "    if min_cluster_size is None:\n",
    "        # your current heuristic (≈1.5% of corpus, at least 200)\n",
    "        min_cluster_size = max(200, math.floor(0.015 * N))\n",
    "\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        n_components=5,\n",
    "        min_dist=min_dist,\n",
    "        metric=\"cosine\",\n",
    "        random_state=random_state,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    hdbscan_model = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method= method,\n",
    "        prediction_data=True,\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "    )\n",
    "\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        token_pattern=r\"(?u)\\b[\\w_]{3,}\\b\",\n",
    "        stop_words=None,\n",
    "        max_df=0.9,\n",
    "    )\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=st_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        language=\"english\",\n",
    "        calculate_probabilities=True,\n",
    "        verbose=True,\n",
    "        top_n_words=top_n_words,\n",
    "    )\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs, embs)\n",
    "\n",
    "    examine_topics(topic_model, topics, probs)\n",
    "    \n",
    "    return topic_model, topics, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7dabb5a5-5680-4f4c-87f5-bec0e489e5f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_samples=25, eps=0.02 -> subtopics=10, noise=58.1%\n",
      "min_samples=30, eps=0.02 -> subtopics=6, noise=52.0%\n",
      "min_samples=35, eps=0.02 -> subtopics=9, noise=59.7%\n",
      "min_samples=40, eps=0.02 -> subtopics=7, noise=51.2%\n"
     ]
    }
   ],
   "source": [
    "# Grid search on epsilon and min sample says for HDBSCAN\n",
    "\n",
    "# reuse docs_big, embs_big, umap_sub, M (len(docs_big))\n",
    "min_cluster_size = max(200, int(0.015* M))  # ~1% of subset size\n",
    "\n",
    "ms_values  = [25,30,35,40]\n",
    "eps_values = [0.02]\n",
    "\n",
    "results = []\n",
    "\n",
    "for ms in ms_values:\n",
    "    for eps in eps_values:\n",
    "        hdb_sub = HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=ms,\n",
    "            metric=\"euclidean\",\n",
    "            cluster_selection_method=\"eom\",\n",
    "            prediction_data=True,\n",
    "            cluster_selection_epsilon=eps\n",
    "        )\n",
    "\n",
    "        vec_sub = CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            token_pattern=r\"(?u)\\b[\\w_]{3,}\\b\",\n",
    "            stop_words=None,\n",
    "            min_df=1,\n",
    "            max_df=1.0\n",
    "        )\n",
    "\n",
    "        sub_model = BERTopic(\n",
    "            umap_model=umap_sub,     # keep your existing UMAP config\n",
    "            hdbscan_model=hdb_sub,\n",
    "            vectorizer_model=vec_sub,\n",
    "            calculate_probabilities=True,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        sub_topics, sub_probs = sub_model.fit_transform(docs_big, embs_big)\n",
    "        sub_info = sub_model.get_topic_info()\n",
    "\n",
    "        n_subtopics = (sub_info.Topic != -1).sum()\n",
    "        noise_share = (np.array(sub_topics) == -1).mean()\n",
    "\n",
    "        results.append((ms, eps, n_subtopics, noise_share))\n",
    "        print(f\"min_samples={ms:2d}, eps={eps:.2f} \"\n",
    "              f\"-> subtopics={n_subtopics}, noise={noise_share:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76f613-57c0-4692-a259-bedbfea207c6",
   "metadata": {},
   "source": [
    "Grid Search Results:\n",
    "\n",
    "min_samples=20, eps=0.00 -> subtopics=12, noise=55.5%\n",
    "min_samples=20, eps=0.02 -> subtopics=12, noise=55.5%\n",
    "min_samples=20, eps=0.05 -> subtopics=12, noise=55.5%\n",
    "min_samples=25, eps=0.00 -> subtopics=11, noise=56.9% min_samples=25, eps=0.02 -> subtopics=10, noise=58.1%\n",
    "min_samples=30, eps=0.02 -> subtopics=6, noise=52.0%\n",
    "min_samples=35, eps=0.02 -> subtopics=9, noise=59.7%\n",
    "min_samples=40, eps=0.02 -> subtopics=7, noise=51.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b47e8a9-fcd0-4982-a90e-c68082e1d293",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs_big' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Grid search on mcs_fracs, ms_values, and epsilon\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m M = \u001b[38;5;28mlen\u001b[39m(\u001b[43mdocs_big\u001b[49m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# SMALL grid (aiming to reduce both cluster count and noise)\u001b[39;00m\n\u001b[32m      6\u001b[39m mcs_fracs = [\u001b[32m0.010\u001b[39m, \u001b[32m0.012\u001b[39m, \u001b[32m0.015\u001b[39m, \u001b[32m0.02\u001b[39m]      \u001b[38;5;66;03m# ≈ 1.0%, 1.2%, 1.5% of subset\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'docs_big' is not defined"
     ]
    }
   ],
   "source": [
    "# Grid search on mcs_fracs, ms_values, and epsilon\n",
    "\n",
    "M = len(docs_big)\n",
    "\n",
    "# SMALL grid (aiming to reduce both cluster count and noise)\n",
    "mcs_fracs = [0.010, 0.012, 0.015, 0.02]      # ≈ 1.0%, 1.2%, 1.5% of subset\n",
    "ms_values = [40, 45, 50]           # around your previous best\n",
    "epsilon   = 0.02                       # mild merge tolerance\n",
    "\n",
    "best = {\"noise\": 1.0, \"k\": 10**9, \"cfg\": None, \"model\": None, \"topics\": None, \"info\": None}\n",
    "\n",
    "for frac in mcs_fracs:\n",
    "    mcs = max(120, int(frac * M))\n",
    "    for ms in ms_values:\n",
    "        k, noise, mdl, t, info = run_once(mcs, ms, epsilon)\n",
    "        # choose best by: lowest noise, then fewer clusters\n",
    "        if (noise < best[\"noise\"]) or (noise == best[\"noise\"] and k < best[\"k\"]):\n",
    "            best.update({\"noise\": noise, \"k\": k, \"cfg\": (mcs, ms, epsilon),\n",
    "                         \"model\": mdl, \"topics\": t, \"info\": info})\n",
    "\n",
    "print(\"\\nBest by (min noise, then fewer clusters):\")\n",
    "print(f\"mcs={best['cfg'][0]}, min_samples={best['cfg'][1]}, eps={best['cfg'][2]:.2f}  \"\n",
    "      f\"→  subtopics={best['k']}, noise={best['noise']:.1%}\")\n",
    "\n",
    "# Keep best handy if you want to inspect words/examples next:\n",
    "best_sub_model  = best[\"model\"]\n",
    "best_sub_topics = best[\"topics\"]\n",
    "best_sub_info   = best[\"info\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebe2aa-fd63-4626-99dd-1b4f620c52f3",
   "metadata": {},
   "source": [
    "Grid Search Results:\n",
    "\n",
    "mcs= 504, min_samples=40, eps=0.02  →  subtopics=12, noise=56.1%\n",
    "mcs= 504, min_samples=45, eps=0.02  →  subtopics=13, noise=62.8%\n",
    "mcs= 504, min_samples=50, eps=0.02  →  subtopics=12, noise=55.4%\n",
    "mcs= 605, min_samples=40, eps=0.02  →  subtopics=10, noise=58.3%\n",
    "mcs= 605, min_samples=45, eps=0.02  →  subtopics=11, noise=65.0%\n",
    "mcs= 605, min_samples=50, eps=0.02  →  subtopics=10, noise=57.6%\n",
    "mcs= 757, min_samples=40, eps=0.02  →  subtopics= 7, noise=51.2%\n",
    "mcs= 757, min_samples=45, eps=0.02  →  subtopics= 2, noise=33.6%\n",
    "mcs= 757, min_samples=50, eps=0.02  →  subtopics=10, noise=57.6%\n",
    "mcs=1009, min_samples=40, eps=0.02  →  subtopics= 7, noise=51.2%\n",
    "mcs=1009, min_samples=45, eps=0.02  →  subtopics= 7, noise=58.7%\n",
    "mcs=1009, min_samples=50, eps=0.02  →  subtopics= 7, noise=46.9%\n",
    "\n",
    "Best by (min noise, then fewer clusters):\n",
    "mcs=757, min_samples=45, eps=0.02  →  subtopics=2, noise=33.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bbb52-dd77-42ad-871a-534d8a80e36e",
   "metadata": {},
   "source": [
    "#### Optimized Run Through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "99215036-4026-4078-830a-3feaefc40d35",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 22:06:53,121 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-06 22:08:04,096 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-06 22:08:04,101 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-06 22:08:18,447 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-06 22:08:18,466 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-09-06 22:08:30,283 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered topics (excl. -1): 19\n",
      "Outlier docs (-1): 31075 / 56574 = 54.9%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>31075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>1784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>1556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>1154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>1141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>1086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count\n",
       "0      -1  31075\n",
       "1       0   2988\n",
       "2       1   2374\n",
       "3       2   2363\n",
       "4       3   1990\n",
       "5       4   1838\n",
       "6       5   1784\n",
       "7       6   1556\n",
       "8       7   1307\n",
       "9       8   1262\n",
       "10      9   1154\n",
       "11     10   1141\n",
       "12     11   1086\n",
       "13     12    781\n",
       "14     13    705\n",
       "15     14    702\n",
       "16     15    652\n",
       "17     16    630\n",
       "18     17    614\n",
       "19     18    572"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full runtime: 1.64 minutes.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit BERTopic model 00\n",
    "\n",
    "start = time.time()\n",
    "    \n",
    "topic_model_00, topics_00, probs_00 = run_bertopic(\n",
    "    docs, embs, st_model,\n",
    "    n_neighbors=15,          # tighter neighborhoods\n",
    "    min_dist=0.08,           # spread clusters out just a bit\n",
    "    min_cluster_size=int(0.01 * len(docs)),  # clusters absorb more points\n",
    "    min_samples=20,          # relax noise labeling\n",
    "    method=\"leaf\"            # keep fine granularity\n",
    ")\n",
    "    \n",
    "end = time.time()\n",
    "    \n",
    "runtime = (end-start)/60\n",
    "print(f\"Full runtime: {runtime:.2f} minutes.\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e08ac106-76e5-4b95-9ad2-0d6afd720fa7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Topic 0 | size=2988\n",
      "\n",
      "Top words: wunder_unders, hotty_hot, liner, waist, high_rise, sizing_help, thigh, waistband, leg, speed_ups, hotty_hots, hip, waist_hip, speed_short, wunder\n",
      "\n",
      "--- Ex1: wunder_unders flattering flattering_make booty pop aligns_wunder train_love want_legging ancient_copper color_look ancient_copper wunder_unders size_fit wunder_unders flattering_legging thank_much advance\n",
      "\n",
      "--- Ex2: hotty_hot high_rise usually_size low_rise hotty_hots recently_bought pair_high rise one felt extremely big happen_else size_size high_rise hotty_hots scared small\n",
      "\n",
      "--- Ex3: hotty_hot brief liner size newbie question_people experience hotty_hot short_true size_sizing hotty_hot short_size despite sizing_find liner tends ride give_wedgie quite_often running_working problem larger butt_hip waist ratio wondering liner brief inch_hotty hots different worth_try tend_prefer hi...\n",
      "================================================================================\n",
      "\n",
      "Topic 1 | size=2374\n",
      "\n",
      "Top words: gift_card, package, card, refund, fedex, email, shipped, shipping, gec, receipt, delivered, customer_service, delivery, sent, cancelled\n",
      "\n",
      "--- Ex1: read waiting gift_card month_ago ordered_aligns pilled bad hour wear gave store_credit gift_card said shipped month_later gift_card still contacted_customer service_said email look_never actually sent ordered_pair wunder_train gift_card colour sent_back processed return may however say_anything emai...\n",
      "\n",
      "--- Ex2: gift_card stolen family bought_gift card earlier month bday gift gave physical_gift card bought_store told_store also associated gift_card went_store use_gift card earlier_week find money person helping called gift_card phone_number look happened gift_card line said contact day resolution looking co...\n",
      "\n",
      "--- Ex3: gift_card stolen customer_service awesome bought wife gift_card bday live mile nearest_store knew wanted browse find ton_stuff checkout gift_card cashier state open gift_card package scan back opened slid denied tried denied shock receipt bought see transaction bank transaction cashier hand phone_nu...\n",
      "================================================================================\n",
      "\n",
      "Topic 2 | size=2363\n",
      "\n",
      "Top words: strawberry_milkshake, color_comparison, comparison, sonic_pink, legacy_green, grey_sage, blue, graphite_grey, request, smoked_spruce, tone, neutral, purple, shade, flush_pink\n",
      "\n",
      "--- Ex1: strawberry_milkshake know possibility bottom strawberry_milkshake love_matching set\n",
      "\n",
      "--- Ex2: strawberry_milkshake sonic_pink another pink wfh look strawberry_milkshake pocc sonic_pink adapted_state jogger\n",
      "\n",
      "--- Ex3: strawberry_milkshake washing strawberry_milkshake hotty_hots wash_color white load wash_color strawberry_milkshake\n",
      "================================================================================\n",
      "\n",
      "Topic 3 | size=1990\n",
      "\n",
      "Top words: scuba, hoodie, full_zip, half_zip, zip, scuba_oversized, funnel_neck, scuba_zip, scuba_hoodie, hoodies, scuba_half, scuba_full, zip_hoodie, xxl, oversized_scuba\n",
      "\n",
      "--- Ex1: could_keep full_zip hoodie\n",
      "\n",
      "--- Ex2: ever half_zip tailored full_zip wondering scuba_half zip professionally altered full_zip much_wear trench half_zip full_zip hot gym\n",
      "\n",
      "--- Ex3: small scuba_collection lavishnesslow small scuba_collection not_pictured full_zip bone full_zip meadowsweet_pink parting half_zip blissful_blue also parting left_right column espresso_cropped stmi cropped solar_orange java_full zip utility_yellow full_zip lemon_sorbet full_zip powder_blue full_zip i...\n",
      "================================================================================\n",
      "\n",
      "Topic 4 | size=1838\n",
      "\n",
      "Top words: bag, belt_bag, backpack, everywhere_belt, ebb, strap, city_adventurer, water_bottle, everywhere_belt bag, carry, pouch, tote, crossbody, mini, tote_bag\n",
      "\n",
      "--- Ex1: crossbody_camera bag_everywhere belt_bag ebb someone either bag read quite review_still decide ideally_want able_use year including canadian_winter main con price colour availability factor crossbody_camera bag easily flip upside spilling content everywhere_belt bag adjustable elastic strap stay_put...\n",
      "\n",
      "--- Ex2: bag tell_name bag purchased canada\n",
      "\n",
      "--- Ex3: use everywhere_belt bag someone convince buy everywhere_belt bag holding use crossbody purse day_not much belt_bag girl sooo cute use put thinking_maybe walking_dog hold_key potty bag edit ordered damn thanks great reply\n",
      "================================================================================\n",
      "\n",
      "Topic 5 | size=1784\n",
      "\n",
      "Top words: jogger, dance_studio, align_jogger, ready_rulu, scuba_jogger, adapted_state, abc_jogger, surge_jogger, dance_studio jogger, abc_pant, slim, commission_pant, slim_fit, rise_jogger, inseam\n",
      "\n",
      "--- Ex1: dance_studio jogger someone_tell dance_studio jogger_sizing lb_wear jean\n",
      "\n",
      "--- Ex2: dance_studio jogger taller girl dance_studio jogger christmas size_fit perfectly except_fact nearly two_inch short cling calf long_leg normal_align legging_fit perfectly pant_similar style dance_studio jogger bit_longer\n",
      "\n",
      "--- Ex3: dance_studio jogger_sizing dance_studio pant reading_review dance_studio jogger seem conflicting sizing_true size_wear aligns dance_studio pant_dance studio_cargo size_size dance_studio jogger tia\n",
      "================================================================================\n",
      "\n",
      "Topic 6 | size=1556\n",
      "\n",
      "Top words: energy_bra, cup, band, cloud_bra, support, flow_bra, bra_size, long_line, boob, high_neck, free_serene, strap, bra_sizing, chest, ebb_street\n",
      "\n",
      "--- Ex1: impulse_purchase high_neck energy_bra wmtm regret high_neck energy_bra cup_love made impulse_purchase morning reading_review wmtm high_neck energy_bra medium_support cup mainly high_neck already regular_energy version read_review lunch break sound good\n",
      "\n",
      "--- Ex2: energy_bra sizing_size energy_bra lb\n",
      "\n",
      "--- Ex3: energy_bra sizing_wondering larger_chest able_wear energy_bra see sized cup energy_bra high_support free_elevated bra_tried invigorate_training tank_big make_sense try energy_bra also_seem fit_different longline high_neck\n",
      "================================================================================\n",
      "\n",
      "Topic 7 | size=1307\n",
      "\n",
      "Top words: outlet, black_friday, boxing_day, outlet_haul, location, app, friday, cyber_monday, tomorrow, weekend, send_sale, selection, outlet_store, haul, ship\n",
      "\n",
      "--- Ex1: black_friday possible cyber_monday drop_discussion everyone_hope everything_wanted black_friday drop_last week_decided create_new answer_commonly asked_question black_friday cyber_monday give place_share haul discus anything_else black_friday cyber_monday cyber_monday drop_drop time happen online_it...\n",
      "\n",
      "--- Ex2: black_friday faq_discussion simple_question everyone_black friday_sale impending decided_create answer_commonly asked_question place_discussion around_anything related_dedicated simple_question next_week please_feel free_black friday_sale start year_drop different previous_year according email least...\n",
      "\n",
      "--- Ex3: black_friday cyber_monday discussion_everyone missed black_friday drop_last night decided_create new answer_commonly asked_question black_friday cyber_monday give place_share haul discus anything_else black_friday related_dedicated simple_question please_feel free looking previous faq found_another ...\n",
      "================================================================================\n",
      "\n",
      "Topic 8 | size=1262\n",
      "\n",
      "Top words: stain, smell, washing, washed, detergent, water, laundry, clean, dryer, dry, bleach, stained, washer, spot, washing_machine\n",
      "\n",
      "--- Ex1: successfully yellow sunscreen stain_white align_tank method run water stained area rub dawn platinum dish_soap directly stain soak whole shirt overnight solution water dish_soap idk concentration used enough_make solution blue ish rinse well lay shirt flat outside sun hr long_enough see bleaching ef...\n",
      "\n",
      "--- Ex2: unknown stain laundry idea stain think_may actually came laundry used cold_water wash laundry cold_water tide wash laundry noticed taking clothes dryer stain brand nee oversized_crew tip_advice appreciated picture stained crew\n",
      "\n",
      "--- Ex3: not repeat not_use dawn_dish soap light_colored nulu_fabric posterity use dawn_dish soap sometimes stain_remover clothing year episode damage left blue dish_soap pink_shirt died blue left overnight leave long use blue soap right well bra kohlrabi color_pink dawn_dish soap supposed gentle hand chain ...\n",
      "================================================================================\n",
      "\n",
      "Topic 9 | size=1154\n",
      "\n",
      "Top words: tee, swiftly_tech, sleeve, cates_tee, race_length, short_sleeve, swiftly, crew, perfectly_oversized, metal_vent, arm, bodysuit, tech, swiftly_tech long_sleeve, wundermost\n",
      "\n",
      "--- Ex1: looking_perfect everyday short_sleeve tshirt going lll_collection realized short_sleeve tee give_advice best_everyday use_align short_looking online back_action love_crew short_sleeve tee\n",
      "\n",
      "--- Ex2: swiftly_relaxed short_sleeve swiftly_tech short_sleeve first_ever swiftly_relaxed short_sleeve shirt mail amazing trying_back shape worn short_sleeved tee since_covid flattering shirt ever four color_want short_sleeve shirt already crazy hot tell swiftly_tech short_sleeve fit_compared relaxed versio...\n",
      "\n",
      "--- Ex3: swiftly_tech race_length measurement owns swiftly_tech short_sleeve race_length size_know measurement pit bottom_hem planning local_store swiftly hemmed really race_length\n",
      "================================================================================\n",
      "\n",
      "Topic 10 | size=1141\n",
      "\n",
      "Top words: align_tank, tank_top, ebb_street, built_bra, cup, high_neck, tank_size, power_pivot, strap, chest, boob, shelf_bra, crop_tank, pad, key_balance\n",
      "\n",
      "--- Ex1: align_tank sizing_size recommend align_tank foot pound flow_bra\n",
      "\n",
      "--- Ex2: align_tank size_tell larger_size released align_tank still_fit cup assuming also_hoping bigger_size work_usually tried_size align_tank fit except boob\n",
      "\n",
      "--- Ex3: align_tank fit_right swiftly_tech shirt advice_size align_tank\n",
      "================================================================================\n",
      "\n",
      "Topic 11 | size=1086\n",
      "\n",
      "Top words: discount, sweat_collective, interview, educator, job, applied, position, email, group_interview, part_time, employee, applying, healthcare_discount, manager, code\n",
      "\n",
      "--- Ex1: attend multiple job interview store okay applied work around august received invite group_interview last_week completed second_interview tuesday yet received offer work_store interested received_email minute_ago store_want work another_store applied hoisting hiring event october respectively since y...\n",
      "\n",
      "--- Ex2: sweat_collective friend_family discount wondering heard going sweat_collective friend_family discount year normally around holiday thanks\n",
      "\n",
      "--- Ex3: using first_responder discount instead sweat_collective sweat_collective first_responder discount purchasing online default discount_sweat collective_discount cannot_use first_responder discount toggle two use_online might_well try_use discount\n",
      "================================================================================\n",
      "\n",
      "Topic 12 | size=781\n",
      "\n",
      "Top words: community, rule, selling, shopping, price, resellers, spending, user, message, fit_pic, employee, company, spend, educator, stop\n",
      "\n",
      "--- Ex1: observation affording original store_employee started replying linked realized_not reply tip_trick shopping reply mostly rein spending also_lot talk lot spending thatapplefarmer asked paraphrased affording reply long winded answer_question people_buying piece_loving well using people_usually vintage...\n",
      "\n",
      "--- Ex2: sub_update call new_sub feedback everyone_wanted give every guy huge thank helping amazing_community reach member since community grown user less half_year since_time done another_state sub wanted_use make important announcement first major announcement looking_add new moderator team obviously sub g...\n",
      "\n",
      "--- Ex3: state_sub rule update change harassment first foremost need address amount harassment receiving late harassment includes not limited messaging arguing decision made creating throwaway calling particular public attempted doxing digging history agree harassment not sub_rule also term use zero toleranc...\n",
      "================================================================================\n",
      "\n",
      "Topic 13 | size=705\n",
      "\n",
      "Top words: luon, always_effortless, luon_define, hooded_define, define, sizing_help, jacket_sizing, nulu, jacket_size, size_define, arm, fitted, define_jacket sizing_help, jacket_nulu, defines\n",
      "\n",
      "--- Ex1: looking_black define_jacket fitted_look stuck material_style please_help hooded_nulu define pitch_grey decided_want another define hug_body ordering_size sure_want define_black unsure material work month_old hooded_nulu define pilled always scary cause damage investment want nulu time scared pilling...\n",
      "\n",
      "--- Ex2: discontinuing luon_define jacket_know discontinuing luon_define jacket_month since_released new_colour tbh someone life colder country luon_material way nulu_luon thicker_warmer great canadian_winter checking_every tuesday_new colour nope nothing colour every_week black_white navy_blue tired happene...\n",
      "\n",
      "--- Ex3: hooded_define nulu design change hello original design hooded_define discontinued replaced double_seam went_store buy hooded_define jacket_nulu slightly cooler day morning nowhere_found available seems_look luon_define jacket double_seam across hood already luon wanted slightly_different look origin...\n",
      "================================================================================\n",
      "\n",
      "Topic 14 | size=702\n",
      "\n",
      "Top words: wunder_puff, vest, cold, rain, cold_weather, coat, running, parka, hiking, temperature, cross_chill, waterproof, layer, warmer, rain_jacket\n",
      "\n",
      "--- Ex1: vest wunder_puff jacket fellow lemonheads cloudscape jacket_last year_know gotten flatter something_keep warm_enough finicky weather guy_think layer vest underneath cop wunder_puff jacket_feel cloudscape loose bottom already size wind get wunder_puff normal_length mid_length\n",
      "\n",
      "--- Ex2: wunder_puff vest cropped_long general thought impression fall_winter anticipation throat decided season wunder_puff think vest cropped_jacket long_jacket every weather contingency covered question though insight people vest cropped wear water repellant water repellant withstand minute light mist goi...\n",
      "\n",
      "--- Ex3: wunder_puff vest random_restock full_size run wunder_puff vest_black\n",
      "================================================================================\n",
      "\n",
      "Topic 15 | size=652\n",
      "\n",
      "Top words: hemming, hemmed, hem, flare, groove_pant, mini_flare, inch, inseam, groove, ankle, hemming_service, hemming_align, align_mini, hemming_question, legging_hemmed\n",
      "\n",
      "--- Ex1: hemming length limit last_week went_local store several_item tag hemmed educator_told new_item accept hemming moreover minimum length hemming inch new regulation\n",
      "\n",
      "--- Ex2: hemming urban_stride shr urban_stride shr_pant hemmed hemming make ankle tapered flair bought_size plan hemmed tiny_bit inch\n",
      "\n",
      "--- Ex3: hemming megathread hem location recommendation hemming faq offer complimentary_hemming top_pant much_cost free item_hemmed receipt bought someone_else resale_app older_item longer_sold yes not_need tag_receipt hemming done need_item hemming work bring_item store_try dressing_room let educator_know h...\n",
      "================================================================================\n",
      "\n",
      "Topic 16 | size=630\n",
      "\n",
      "Top words: ootd, gym_fit, gym, outfit, high_rise, align_short, educator_ootd, flow_bra, legacy_green, asymmetrical_yoga, align_neck, energy_bra, sunday, velvet_dust, asymmetrical_bra\n",
      "\n",
      "--- Ex1: size_gym work ootds gym scuba_oversized full_zip size_xxl windmill bend_scoop square_bra size windmill align_pant pocket_size oasis_blue back_life sport_bottle blue_linen work scuba_oversized full_zip size_xxl windmill wundermost_ultra soft_nulu hip_length crew_short sleeve_shirt size_true navy grea...\n",
      "\n",
      "--- Ex2: gym_fit check energy_bra true_navy align_short black\n",
      "\n",
      "--- Ex3: ootd gym_fit faves color\n",
      "================================================================================\n",
      "\n",
      "Topic 17 | size=614\n",
      "\n",
      "Top words: underwear, thong, pair_legging, best_legging, compression, undies, gym, wear_legging, running, butt, seamless, squat_proof, legging_best, atrp, underwear_line\n",
      "\n",
      "--- Ex1: short lining underwear hey bit_weird question_looking underwear similar liner speed_short hate underwear working reason liner speed_ups comfortable_thing planet course wear exclusively speed_ups every_time work unfortunately wondering_advice finding underwear level comfort breathability underwear pr...\n",
      "\n",
      "--- Ex2: kind underwear wearing haha title might come strange besides underwear thong kind brand wearing_aligns constantly switching three comfort strength_training cardio outline seamless target brand thong cheeky underwear seems appear matter many_time adjust please_help sincerely girl trying_avoid yeast i...\n",
      "\n",
      "--- Ex3: underwear yes middle_school daughter love mostly top hotty_hots everytime try_legging underwear screaming know_not feel_comfortable going without underwear wearing thong found underwear work_wear legging\n",
      "================================================================================\n",
      "\n",
      "Topic 18 | size=572\n",
      "\n",
      "Top words: skirt, court_rival, pace_rival, court_rival skirt, align_dress, tennis_skirt, pace_rival skirt, align_skirt, tennis, skirt_size, play_pleat, high_rise, rival_skirt, brunch_back, skirt_fit\n",
      "\n",
      "--- Ex1: court_rival skirt discontinued court_rival skirt dropped not_seeing anywhere app favorite athletic skirt hope_not gone_good\n",
      "\n",
      "--- Ex2: ootd_black court_rival skirt\n",
      "\n",
      "--- Ex3: pace_rival skirt obsessed pace_rival skirt wish oasis_blue\n",
      "\n",
      "Non-noise topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "# Check top words\n",
    "\n",
    "check_top_words(topic_model_00, topics_00, probs_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "264a6f3d-5ff1-4f79-b17f-ff9579d3b579",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic diversity (for top 10 words for each topic): 1.000\n"
     ]
    }
   ],
   "source": [
    "# Check topic diversity\n",
    "\n",
    "TOP_N = 10\n",
    "words_per_topic = {\n",
    "    t: [w for w, _ in topic_model.get_topic(t)[:TOP_N]]\n",
    "    for t in topic_info[\"Topic\"].tolist() if t != -1\n",
    "}\n",
    "\n",
    "all_top_words = list(chain.from_iterable(words_per_topic.values()))\n",
    "diversity = len(set(all_top_words)) / max(1, len(all_top_words))\n",
    "print(f\"Topic diversity (for top {TOP_N} words for each topic): {diversity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e5e8e-0060-47e8-b8df-e8c2eb4f0304",
   "metadata": {},
   "source": [
    "## Topic Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88292876-71eb-464e-b5b8-d82015cb7c83",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Overall map of topics\n",
    "\n",
    "topic_model_00.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66c4da09-3e1b-4b0c-a40e-f18ccf06adfe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualize top words per topic (bar charts)\n",
    "\n",
    "topic_model_00.visualize_barchart(top_n_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "334e3538-8eb0-4793-90e1-6ad1a851f921",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualizie topic similarity heatmap\n",
    "\n",
    "topic_model_00.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82a33566-3d9b-448f-b28a-fb2c01b69d12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualize hierarchy (dendrogram)\n",
    "\n",
    "topic_model_00.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b428b22-6833-4715-8b63-d80264f40e35",
   "metadata": {},
   "source": [
    "## Save topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a585b3d3-3208-42ff-b362-caffab1941f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Add topic labels and probabilities to original dataframe \n",
    "\n",
    "lulu_df[\"topic_00\"] = topics_00\n",
    "lulu_df[\"topic_prob_00\"] = probs_00.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5300b5f1-35bb-4e77-9910-8ab58285459a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Save as parquet\n",
    "\n",
    "lulu_df.to_parquet(f\"{PATH}/lulu_df_with_topics_00.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lulu_bert_env)",
   "language": "python",
   "name": "lulu_bert_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
